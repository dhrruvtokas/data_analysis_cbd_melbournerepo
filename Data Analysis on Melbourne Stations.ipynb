{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Author: Dhrruv Tokas\n",
    "#### Email ID: dhrruvtokas@gmail.com\n",
    "\n",
    "Date: 25/10/2020\n",
    "\n",
    "Environment: Python 3.6.0 and Anaconda 4.3.0 (64-bit)\n",
    "\n",
    "Main Libraries used:\n",
    "* regex 2020.7.14 (mainly for regular expression, included in Anaconda Python 3.6) \n",
    "* pandas 1.0.1 (mainly for csv operations, included in Anaconda Python 3.6) \n",
    "* nltk 3.4.5 (mainly for Analysis, included in Anaconda Python 3.6) \n",
    "* numpy 1.18.1 (mainly for arrays and math operations, included in Anaconda Python 3.6) \n",
    "* matplotlib 3.3.2 (mainly for plotting visualizations, included in Anaconda Python 3.6) \n",
    "* sklearn 0.22.1 (mainly for linear regression, included in Anaconda Python 3.6) \n",
    "* xmltodict 0.12.0 (mainly used for XML related operations, included in Anaconda Python 3.6)\n",
    "* tabula-py 1.4.1 (mainly used for PDF related operations, included in Anaconda Python 3.6)\n",
    "* geopandas 0.8.1 (mainly used for shape files, included in Anaconda Python 3.6)\n",
    "* beautifulsoup4 4.9.3 (mainly used for , included in Anaconda Python 3.6)\n",
    "* html5lib 1.1 (mainly used for HTML related operations, included in Anaconda Python 3.6)\n",
    "\n",
    "Platform Used:\n",
    "* Microsoft Windows 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: DATA INTEGRATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "This analaysis focuses on `Data Integration` and `Data Reshaping`, and for the same purpose I was provided 7 differnt datasets including  hospitals,\n",
    "supermarkets, shopping centers, real estate which needs to be grouped together in a single dataset. The dataset (all together) have a total of `21 different attributes` containing several tuples which will be displayed later for the same.\n",
    "\n",
    "* Tasks that were performed:\n",
    "1. Data Integration: The provided seven datasets were combined into a single dataset [`Section 7`]\n",
    "2. Data Data Reshaping: Several transformation/normalisation methods (`Root Tranformation`, `Power Tranformation`, `Log Transfomation`, `Z-Score Tranformation`, and `MinMax Tranformation`) were briefly studied and explained [`Section 8`]. Then the resulting values were then used to `predict` the `property price` of each property by using a `Linear Model`.\n",
    "\n",
    "* Conclusion:\n",
    "All the process that have been done in order to get the the final ouput file (`dhrruvtokas_solution.csv`). The `property price` of each property was succesfully `predicted` in the later sections.\n",
    "\n",
    "* References: Sources which have been used as a reference to complete this assignmnet\n",
    "\n",
    "Other details are thoroughly provided in the following sections of the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Importing Libraries\n",
    "In this specific section, all the required `libraries` will be imported so these `packages` can be used later for the `Data Integration` and `Data Reshaping`. These libraries were used throughout the following sub-sections, each of these libraries serves their own `distinct` purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # Will be used for dataframes\n",
    "from zipfile import ZipFile # Will be used for extracting datasets\n",
    "import glob # Will be used to retrieve matchine filenames\n",
    "import shutil # Will be used for file operations\n",
    "import os # Will be used to change and traverse through directories\n",
    "import re # Will be used for regular expression operations\n",
    "import json # Will be used to read json files\n",
    "import xmltodict # Will be used for converting xml into dictionary\n",
    "import xml.etree.ElementTree as ET # Will be used for converting xml into dataframe\n",
    "import tabula # Will be used for coverting pdf into a dataframe\n",
    "from bs4 import BeautifulSoup as bs # Will be used for extracting xml tags\n",
    "from itertools import zip_longest # Will be used to to iterate\n",
    "import pysal as ps # Will be used to read the dbf file\n",
    "import shapefile # Will  be used to read shp file\n",
    "import geopandas as gpd # Will be used to read shp file\n",
    "from shapely.geometry import MultiPolygon # Will be used to plot polygon coordinates\n",
    "import matplotlib.pyplot as plt # Will be used to plot\n",
    "import numpy as np # Will be used for shapefile\n",
    "import math # Will be used for calculating haversine distance\n",
    "from math import sin, cos, sqrt, atan2, radians, ceil # Will be used for calculate distance between two coordinates\n",
    "from shapely.geometry import MultiPoint, Point, Polygon # Will be used for shapefile\n",
    "from sklearn import preprocessing # Will be used for transformation\n",
    "from sklearn.linear_model import LinearRegression # Will be used to create a linear model for predictions\n",
    "from sklearn.model_selection import train_test_split # Will be used to train the data after applying transformation techniques\n",
    "from scipy import stats # Will be used for transformation\n",
    "import seaborn as sns # Will be used to plot distplot\n",
    "# import pycrs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Extracting The Required Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Extracting The Student Dataset\n",
    "The dataset named `dhrruvtokas.zip`was extracted and all of its sub-datasets was used as a part for the final dataset. A `ZipFile()` function was used to read the `dhrruvtokas.zip` dataset and a `extractall()` function was used to extract all those files which were present in this dataset. After the available files were extracted, the same dataset was later closed using a `close()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_data_name = \"dhrruvtokas.zip\" # Will store the name of the dataset\n",
    "with ZipFile(student_data_name, 'r') as zip: # Will open the zip file to read\n",
    "    print(\"Extracted Sub-datasets:\\n\") # Will display a label for the output\n",
    "    zip.printdir() # Will display the sub-datasets which were extracted\n",
    "    zip.extractall() # Will extract the sub-datasets\n",
    "    zip.close() # Will close the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Extracting The GTFS Melbourne Train Information Dataset\n",
    "The dataset named `GTFS_Melbourne_Train_Information.zip` was extracted and all of its sub-datasets was used as a part for the final dataset. A `ZipFile()` function was used to read the `GTFS_Melbourne_Train_Information.zip` dataset and a `extract()` function was used to extract all those files which were present in this dataset. After the available files were extracted, the same dataset was later closed using a `close()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_directory = os.getcwd() # Will retrieve and store the current working directory\n",
    "gtfs_data_name = \"GTFS_Melbourne_Train_Information.zip\" # Will store the name of the dataset\n",
    "print(\"Extracted Sub-datasets:\\n\") # Will display a label for the output\n",
    "with ZipFile(gtfs_data_name) as zip: # Will open the zip file to read\n",
    "    for zip_info in zip.infolist(): # Will return the zip_info objects\n",
    "        if zip_info.filename[-1] == '/': # Will be used to recursively extract the sub-datasets\n",
    "            continue\n",
    "        zip_info.filename = os.path.basename(zip_info.filename) # Will extract the name of the sub-datasets\n",
    "        zip.extract(zip_info, current_directory) # Will extract the sub-datasets into the main working directory\n",
    "        print(zip.extract(zip_info, current_directory)) # Will display the name of each sub-dataset which was extracted\n",
    "    zip.close() # Will close the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Extracting The Victoria Suburb Boundary Dataset\n",
    "The dataset named `vic_suburb_boundary.zip` was extracted and all of its sub-datasets was used as a part for the final dataset. A `ZipFile()` function was used to read the `vic_suburb_boundary.zip` dataset and a `extractall()` function was used to extract all those files which were present in this dataset. After the available files were extracted, the same dataset was later closed using a `close()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suburb_data_name = \"vic_suburb_boundary.zip\" # Will store the name of the dataset\n",
    "with ZipFile(suburb_data_name, 'r') as zip: # Will open the zip file to read\n",
    "    print(\"Extracted Sub-datasets:\\n\") # Will display a label for the output\n",
    "    zip.printdir() # Will display the sub-datasets which were extracted\n",
    "    zip.extractall() # Will extract the sub-datasets\n",
    "    zip.close() # Will close the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Loading the Student Dataset \n",
    "The sub-datasets of the dataset named `dhrruvtokas.zip` which was extracted in the `section 3.1` were read and displayed in the following sub-sections and these sub-datasets were `hospitals.xlsx`, `real_state.json`, `real_state.xlsx` ,`shoppingcenter.pdf`, and `supermarkets.html` respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Displaying Hospitals Sub-dataset\n",
    "The following section examined and displayed the `hospitals.xlsx` sub-dataset by using a `read_excel()` function of the `pandas` package. In addition to this, an unnecessary column named `Unnamed: 0` (`index column`) were then `dropped` to preserve the original sub-dataset, and then the sub-dataset stored in a dataframe was displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None) # Will set the maximum rows to none \n",
    "hospital_read_data = pd.read_excel('hospitals.xlsx') # Will read the excel file\n",
    "hospital_read_data = hospital_read_data.drop(['Unnamed: 0'],axis=1) # Will remove the unwanted index column\n",
    "hospital_read_data # Will display the excel file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Displaying The Real State Sub-dataset - JSON\n",
    "The following section examined and displayed the `real_state.json` sub-dataset by using a `read_json()` function, and then the sub-dataset stored in a dataframe was displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_state_json_data = pd.read_json(r'real_state.json') # Will load the matching (real state json dataset\n",
    "real_state_json_data # Will display the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Displaying The Real State Sub-dataset - XML\n",
    "The following section examined (by using a `open()` function) and displayed (by using a `read()` function) the `real_state.xml` sub-dataset. After the sub-dataset was read and displayed, the `XML` was then closed using a `close()` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_state_xml_open = open(\"real_state.xml\", \"r\") # Will open the real state xml file to read\n",
    "real_state_xml_read = real_state_xml_open.read() # Will read the xml file\n",
    "real_state_xml_open.close() # XML file will be closed\n",
    "real_state_xml_read # Will display the contents of the xml file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1 Cleaning The Real State XML File\n",
    "The `real state xml` dataset which was extracted in the section above contained some `invalid charaters` such as `b and '` at the beginning of the file as well as at the end of the file. These errors were removed using the `slicing method` so the extracted dataset can be parsed into a dataframe. After those `invalid characters` were sliced, the original dataset (stored inside a dataframe named `real_state_xml_data`) was displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_state_xml_read = \"\".join(real_state_xml_read) # Will covert the xml into string\n",
    "real_state_xml_data = real_state_xml_read[2:-1] # Will remove the invalid xml characters\n",
    "real_state_xml_data # Will display the valid xml data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2 Finding The Names Of The Available Columns In The Real State Dataset - XML\n",
    "A list of available columns in the `real state dataset` were extracted in the following section, this operation was carried by using the `BeautifulSoup` pacakge. Then a `find()` function was used to find the `root node` of the `XML` file, a list named `real_state_attributes` was also used to store the name of its columns, and at the end, the total number of columns was then displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = bs(real_state_xml_data, \"lxml\") # Will load the xml dataset in the soup function\n",
    "tag_name = soup.find('root') # Will find the root node\n",
    "children = tag_name.children # Will be used to find the children\n",
    "print(\"Available Attributes:\\n\") # Will display a label for the output\n",
    "count = 0 # Will keep a count on the number of attributes\n",
    "real_state_attributes = [] # Will create an empty list to store the column names\n",
    "while True: # While children (tag name) exists\n",
    "    try: # For each existing tag name\n",
    "        print(next(children).name) # Will display the attributes\n",
    "        count = count + 1 # Will increment the count by 1\n",
    "    except: # If there is no attribute left\n",
    "        break # Exit\n",
    "print(\"\\nTotal Attributes: \", count) # Will display the number of attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.3 Converting The Real State Sub-dataset Into A Dataframe - XML\n",
    "The Real State XML Sub-dataset which was extracted in the section above will be converted into a dataframe in the following section. The attribute names which were found out in the previous section will be used (by uisng the `findAll()` function) in this section and the data that they carry will be extracted using a combinaton of `BeautifulSoup` package and `re` package. The `Regular Expression` `r\"n:?\\d{1,6}\"` used in the following section work as follows:\n",
    "* n:? = Will check if the matching pattern starts with the n letter\n",
    "* \\d{1,6} =  Will check if the matching pattern is a number of maximum 6 digits\n",
    "The extracted dataset was then stored into a dataframe named `real_state_xml_data_final` which was displayed at the end of this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = bs(real_state_xml_data, \"lxml\") # Will load the xml dataset in the soup function\n",
    "property_id = soup.findAll('property_id') # Will find the property_id tag\n",
    "latitude = soup.findAll('lat') # Will find the lat tag\n",
    "longitude = soup.findAll('lng') # Will find the lng tag\n",
    "address_street = soup.findAll('addr_street') # Will find the addr_street tag\n",
    "price = soup.findAll('price') # Will find the price tag\n",
    "property_type = soup.findAll('property_type') # Will find the property_type tag \n",
    "year = soup.findAll('year') # Will find the year tag\n",
    "bedrooms = soup.findAll('bedrooms') # Will find the bedrooms tag\n",
    "bathrooms = soup.findAll('bathrooms') # Will find the bathrooms tag\n",
    "parking_space = soup.findAll('parking_space') # Will find the parking_space tag\n",
    "\n",
    "property_list = [] # Will create an empty list which will be used to store the property ids\n",
    "latitude_list = [] # Will create an empty list which will be used to store the latitude values\n",
    "longitude_list = [] # Will create an empty list which will be used to store the longitude values\n",
    "address_street_list = [] # Will create an empty list which will be used to store the addresses\n",
    "price_list = [] # Will create an empty list which will be used to store the prices\n",
    "property_type_list = [] # Will create an empty list which will be used to store the property types\n",
    "year_list = [] # Will create an empty list which will be used to store the years\n",
    "bedrooms_list = [] # Will create an empty list which will be used to store the number of bedrooms\n",
    "bathrooms_list = [] # Will create an empty list which will be used to store the number of bathrooms\n",
    "parking_space_list = [] # Will create an empty list which will be used to store the number of parking spaces\n",
    "\n",
    "pattern = r\"n:?\\d{1,6}\" # A pattern which will be matched to extract the required data\n",
    "\n",
    "for prop, lat, long, addr_st, price_amt, prop_typ, year_no, bed_no, bath_no, park_sp  in zip_longest(property_id, latitude, longitude, address_street, price, property_type, year, bedrooms, bathrooms, parking_space): # For each value of all the attributes in the dataset\n",
    "    property_list.append([row.text for row in prop.findAll(re.compile(pattern))]) # Will find the matching pattern and will store the property id in the list\n",
    "    latitude_list.extend([row.text for row in lat.findAll(re.compile(pattern))]) # Will find the matching pattern and will store the latitude value in the list\n",
    "    longitude_list.extend([row.text for row in long.findAll(re.compile(pattern))]) # Will find the matching pattern and will store the longitude value in the list\n",
    "    address_street_list.extend([row.text for row in addr_st.findAll(re.compile(pattern))]) # Will find the matching pattern and will store the addresses in the list\n",
    "    price_list.extend([row.text for row in price_amt.findAll(re.compile(pattern))]) # Will find the matching pattern and will store the prices in the list\n",
    "    property_type_list.extend([row.text for row in prop_typ.findAll(re.compile(pattern))]) # Will find the matching pattern and will store the property types in the list\n",
    "    year_list.extend([row.text for row in year_no.findAll(re.compile(pattern))]) # Will find the matching pattern and will store the year in the list\n",
    "    bedrooms_list.extend([row.text for row in bed_no.findAll(re.compile(pattern))]) # Will find the matching pattern and will store the property id in the list\n",
    "    bathrooms_list.extend([row.text for row in bath_no.findAll(re.compile(pattern))]) # Will find the matching pattern and will store the property id in the list\n",
    "    parking_space_list.extend([row.text for row in park_sp.findAll(re.compile(pattern))]) # Will find the matching pattern and will store the property id in the list\n",
    "    \n",
    "\n",
    "property_list = property_list[0] # Will extract the sublist and the sublist will override the main list\n",
    "print(len(property_list), len(latitude_list), len(longitude_list), len(address_street_list), len(price_list), len(property_type_list), len(year_list), len(bedrooms_list), len(bathrooms_list), len(parking_space_list)) # Will display the number of values in each column\n",
    "print(property_list[0], latitude_list[0], longitude_list[0], address_street_list[0], price_list[0], property_type_list[0], year_list[0], bedrooms_list[0], bathrooms_list[0], parking_space_list[0]) # Will display the first row of each column\n",
    "\n",
    "# Will create a dataframe containing the extracted dataset\n",
    "real_state_xml_data_final = pd.DataFrame({'property_id': property_list, 'lat': latitude_list, 'lng': longitude_list, 'addr_street': address_street_list, 'price': price_list, 'property_type': property_type_list, 'year': year_list, 'bedrooms': bedrooms_list, 'bathrooms': bathrooms_list, 'parking_space': parking_space_list})\n",
    "real_state_xml_data_final # Will Display The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Converting The Shopping Centres SubDatset Into A Dataframe\n",
    "In the following section, the `shopingcenters.pdf` sub-dataset was converted into as dataframe using the `tabula` package. The file was first read by using a `read_pdf()` function and was set to display the records present in all of the pages of this `pdf` formatted file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shopping_centers_pdf = tabula.read_pdf('shopingcenters.pdf', pages=\"all\") # Will convert the pdf file into a dataframe\n",
    "shopping_centers_fix_index = shopping_centers_pdf.drop(['Unnamed: 0'],axis=1) # Will remove the extra index column\n",
    "shopping_centers_part_a = shopping_centers_fix_index[0:49] # Will extract the table from the table from the first page\n",
    "shopping_centers_part_b = shopping_centers_fix_index[50:98] # Will extract the table from the table from the second page\n",
    "shopping_centers_part_c = shopping_centers_fix_index[99:122] # Will extract the table from the table from the third page\n",
    "shopping_centers_part_ab = shopping_centers_part_a.append(shopping_centers_part_b) # Will combine page 1 and 2\n",
    "shopping_centers_part_abc = shopping_centers_part_ab.append(shopping_centers_part_c) # Will combine all pages\n",
    "shopping_centers_data = shopping_centers_part_abc.reset_index(drop=True) # Will reset the dataframe index\n",
    "shopping_centers_data # Will display the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Converting The Supermarkets Sub-Dataset Into A Dataframe\n",
    "In the following section, the `supermarkets.html` dataset was converted into as dataframe using the pandas package. The dataset was first read by using a `read_html()` function. In addition to this, an unnecessary column named `Unnamed: 0` (`index column`) was also dropped to preserve the orginal structure of this dataset. Then the dataset stored inside a dataframe named `supermarkets_data` was displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supermarkets_html = pd.read_html('supermarkets.html') # Will convert the html file into a dataframe\n",
    "supermarkets_data = supermarkets_html[0] # Will convert the list into a dataframe\n",
    "supermarkets_data = supermarkets_data.drop(['Unnamed: 0'],axis=1) # Will remove the unwanted index column\n",
    "supermarkets_data # Will display the supermarkets dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Loading The GTFS Melbourne Train Information Dataset \n",
    "In the following sub-sections, the sub-datasets of a dataset named`GTFS_Melbourne_Train_Information.zip` were extracted and displayed. There were a total of `8 text files` which were `agency.txt`, `calendar.txt`, `calendar_dates.txt`, `routes.txt`, `shapes.txt`, `stop_times.txt`, `stops.txt`, and `trips.txt` respectively. All of these sub-datasets were then read and displayed in the following sub-sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Extracting The Agency Sub-dataset\n",
    "The `agency.txt` subdataset was extracted in the following section and the resulting output was stored and displayed in a dataframe called `agency_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agency_data = pd.read_csv(\"agency.txt\") # Will read the agency dataset\n",
    "agency_data # Will display the agency dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Extracting The Calendar Sub-dataset\n",
    "The `calendar.txt` subdataset was extracted (by using a `read_csv()` function of the `pandas package`) in the following section and the resulting output was stored and displayed in a dataframe called `calendar_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar_data = pd.read_csv(\"calendar.txt\") # Will read the calendar dataset\n",
    "calendar_data # Will display the calendar dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Extracting The Calendar Dates Sub-dataset\n",
    "The `calendar_dates.txt` subdataset was extracted (by using a `read_csv()` function of the `pandas package`) in the following section and the resulting output was stored and displayed in a dataframe called `calendar_dates_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar_dates_data = pd.read_csv(\"calendar_dates.txt\") # Will read the calendar dates dataset\n",
    "calendar_dates_data # Will display the calendar dates dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Extracting The Routes Sub-dataset\n",
    "The `routes.txt` subdataset was extracted (by using a `read_csv()` function of the `pandas package`) in the following section and the resulting output was stored and displayed in a dataframe called `routes_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "routes_data = pd.read_csv(\"routes.txt\") # Will read the routes dataset\n",
    "routes_data # Will display the routes dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Extracting The Shapes Sub-dataset\n",
    "The `shapes.txt` subdataset was extracted (by using a `read_csv()` function of the `pandas package`) in the following section and the resulting output was stored and displayed in a dataframe called `shapes_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes_data = pd.read_csv(\"shapes.txt\") # Will read the shapes dataset\n",
    "shapes_data.head(100) # Will display the first 100 rows shapes dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 Extracting The Stops Sub-dataset\n",
    "The `stops.txt` subdataset was extracted (by using a `read_csv()` function of the `pandas package`) in the following section and the resulting output was stored and displayed in a dataframe called `stops_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops_data = pd.read_csv(\"stops.txt\") # Will read the stops dataset\n",
    "stops_data.head(100) # Will display the first 100 rows stops dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7 Extracting The Stop Times Sub-dataset\n",
    "The `stop_times.txt` subdataset was extracted (by using a `read_csv()` function of the `pandas package`) in the following section and the resulting output was stored and displayed in a dataframe called `stop_times_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_times_data = pd.read_csv(\"stop_times.txt\") # Will read the stops times dataset\n",
    "stop_times_data.head(100) # Will display the first 100 rows of the stops times dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.8 Extracting The Trips Sub-dataset\n",
    "The `trips.txt` subdataset was extracted (by using a `read_csv()` function of the `pandas package`) in the following section and the resulting output was stored and displayed in a dataframe called `trips_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_data = pd.read_csv(\"trips.txt\") # Will read the trips dataset\n",
    "trips_data.head(100) # Will display the first 100 rows of the trips dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Loading The Victoria Suburb Boundary Dataset\n",
    "All of the sub-datasets of a dataset named `vic_suburb_boundary.zip` were extracted in the following sections, there were a total of `4 shape files` which included `VIC_LOCALITY_POLYGON_shp.dbf`, `VIC_LOCALITY_POLYGON_shp.prj`, `VIC_LOCALITY_POLYGON_shp.shp`, `VIC_LOCALITY_POLYGON_shp.shx` respetively. All of these sub-datasets were then read and displayed in the following sub-sections. The main purpose of reading these `shape` files was just to review these datasets, how they look inside a dataframe, and this is precisely why all of these `shape` files were read individually instead of just reading the `VIC_LOCALITY_POLYGON_shp` file. If I had not mentioned any `extension`, then the data from all these `shape` files would have been ready automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Extracting The VIC Locality Polygon Shp Dataset - dbf\n",
    "In the following section, `VIC_LOCALITY_POLYGON_shp.dbf` sub-dataset was extracted and later displayed. A function named `dbf_to_df` was used to convert the input `dbf` file into a `pandas` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dbf_to_df(dbf_file_name): # A function which will will covert dbf into a dataframe\n",
    "    \n",
    "    dbf_open = ps.lib.io.open(dbf_file_name) # Will open the file to read using pysal package\n",
    "    dbf_dictionary = {each_column: dbf_open.by_col(each_column) for each_column in dbf_open.header} # Will convert the db file into a dictionary\n",
    "    dbf_data_frame = pd.DataFrame(dbf_dictionary) # Will convert the dictionary into a dataframe\n",
    "    dbf_open.close() # Will close the file\n",
    "    return dbf_data_frame # Will return the converted dataframe\n",
    "\n",
    "dbf_data = dbf_to_df('VIC_LOCALITY_POLYGON_shp.dbf') # Will specify the dbf file to read\n",
    "dbf_data # Will display the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Extracting The VIC Locality Polygon Shp Dataset - prj\n",
    "In the following section, `VIC_LOCALITY_POLYGON_shp.prj` sub-dataset was extracted and later displayed. A package named `shapefile` was used to read the dataset for the ditsribution it had."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prj_open = open(\"VIC_LOCALITY_POLYGON_shp.prj\", 'r') # Will open the prj dataset as the plain text file\n",
    "prj_read = prj_open.readlines() # Will read the prj file\n",
    "prj_sf_read = shapefile.Reader(\"VIC_LOCALITY_POLYGON_shp.prj\") # Will read the prj shapefile\n",
    "\n",
    "print(prj_read) # Will display the file\n",
    "print(\"\\n\", prj_sf_read) # Will display the data distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Extracting The VIC Locality Polygon Shp Dataset - shp\n",
    "In the following section, `VIC_LOCALITY_POLYGON_shp.shp` sub-dataset was extracted and later displayed. A function named `shp_to_df` was used to convert the input `shp` file into a `pandas` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shp_to_df(shp_file_name): # A function which will will covert dbf into a dataframe\n",
    "\n",
    "    shp_read = shapefile.Reader(shp_file_name) # Will read the shp file\n",
    "    shp_columns = [x[0] for x in shp_read.fields][1:] # Will extract the columns\n",
    "    shp_rows = shp_read.records() # Will extract the rows\n",
    "    shp_coords = [each_sp.points for each_sp in shp_read.shapes()] # For each shape\n",
    "    shp_data_frame = pd.DataFrame(columns=shp_columns, data=shp_rows) # Will create a dataframe and insert extracted columns and rows\n",
    "    shp_data_frame = shp_data_frame.assign(coords=shp_coords) # Will assign coordinates\n",
    "    return shp_data_frame # Will return the dataframe\n",
    "\n",
    "shp_data = shp_to_df('VIC_LOCALITY_POLYGON_shp.shp') # Will specify the shape file to read\n",
    "shp_data.head(100) # Will display the first 100 rows of the shape file dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Extracting The VIC Locality Polygon Shp Dataset - shx\n",
    "In the following section, `VIC_LOCALITY_POLYGON_shp.shx` sub-dataset was extracted and later displayed. The dataset was read by using the `geopandas` package, it was tough part to install this package as it had some conflicts with the other packages but I was able to do it by creating a `new environment` and then installing the required packages in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shx_read = gpd.read_file(\"VIC_LOCALITY_POLYGON_shp.shx\") # Will read the shx file using the geopandas package\n",
    "shx_read.head(100) # Will display the first 100 rows of the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 Type Of Shape Files\n",
    "The following section displayed the type of the `shape files` which were used for this task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shx_read.geom_type # Will display the type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6 Projection Of The Data\n",
    "Since the geopandas reads the data from all of the shape files, `shx_read` can be used to display the projection of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shx_read.crs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Data Integration\n",
    "In the following sections, the datasets which were extracted in the sections will be combined into a single output dataset which will be `csv file`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Extracting and Combining Property Ids\n",
    "In this specific section, the project id column (attribute) was constructed by extracting all the unique `property ids` from the required datasets such as ids from `real_state_json_data`, `real_state_xml_data_final`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will extract the property ids (int) and latitude values (float) from the json as well as xml file and will store the result into a dataframe\n",
    "json_prop = pd.DataFrame({'property_id': real_state_json_data['property_id'].astype(int)})\n",
    "xml_prop = pd.DataFrame({'property_id': real_state_xml_data_final['property_id'].astype(int)})\n",
    "\n",
    "json_dataframe_prop = json_prop.copy() # Will create a copy of the json dataframe\n",
    "xml_dataframe_lat = xml_prop.copy() # Will create a copy of the xml dataframe\n",
    "\n",
    "prop_sorted = pd.concat([json_dataframe_prop,xml_dataframe_lat]) # Will combine both the dataframes\n",
    "prop_sorted.drop_duplicates(subset =\"property_id\", keep = False, inplace = True) # Will drop the rows accoording to the duplicated property ids\n",
    "#prop_sorted.sort_values(['property_id'],inplace=True) # Will sort the rows according to the property ids\n",
    "prop_sorted.reset_index(drop=True,inplace=True) # Will rest the index values to show accurate values\n",
    "\n",
    "prop_id_final = prop_sorted['property_id'] # Will store the property ids\n",
    "\n",
    "prop_sorted # Will display the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Extracting and Combining Latitude Values\n",
    "In this specific section, the `lat` column (attribute) was constructed by extracting all the `latitude values` from the required datasets such as values from `real_state_json_data`, `real_state_xml_data_final`. The final output was then displayed using a dataframe named `json_prop_lat_sorted`. The `property id` of each property was used a basis to extract the `latitude value` of that specific property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will extract the property ids (int) and latitude values (float) from the json as well as xml file and will store the result into a dataframe\n",
    "json_prop_lat = pd.DataFrame({'property_id': real_state_json_data['property_id'].astype(int), 'lat': real_state_json_data['lat'].astype(float)})\n",
    "xml_prop_lat = pd.DataFrame({'property_id': real_state_xml_data_final['property_id'].astype(int), 'lat': real_state_xml_data_final['lat'].astype(float)})\n",
    "\n",
    "json_dataframe = json_prop_lat.copy() # Will create a copy of the json dataframe\n",
    "xml_dataframe = xml_prop_lat.copy() # Will create a copy of the xml dataframe\n",
    "\n",
    "prop_lat_sorted = pd.concat([json_dataframe,xml_dataframe]) # Will combine both the dataframes\n",
    "prop_lat_sorted.drop_duplicates(subset =\"property_id\", keep = False, inplace = True) # Will drop the rows accoording to the duplicated property ids\n",
    "#prop_lat_sorted.sort_values(['property_id'],inplace=True) # Will sort the rows according to the property ids\n",
    "prop_lat_sorted.reset_index(drop=True,inplace=True) # Will rest the index values to show accurate values\n",
    "\n",
    "prop_lat_final = prop_lat_sorted['lat'] # Will only store the latitude values\n",
    "prop_lat_sorted # Will display the dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Extracting and Combining Longitude Values\n",
    "In this specific section, the `lng` column (attribute) was constructed by extracting all the `longitude values` from the required datasets such as values from `real_state_json_data`, `real_state_xml_data_final`. The final output was then displayed using a dataframe named `json_prop_long_sorted`. The `property id` of each property was used a basis to extract the `longitude value` of that specific property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will extract the property ids (int) and longitude values (float) from the json as well as xml file and will store the result into a dataframe\n",
    "json_prop_long = pd.DataFrame({'property_id': real_state_json_data['property_id'].astype(int), 'lng': real_state_json_data['lng'].astype(float)})\n",
    "xml_prop_long = pd.DataFrame({'property_id': real_state_xml_data_final['property_id'].astype(int), 'lng': real_state_xml_data_final['lng'].astype(float)})\n",
    "\n",
    "json_dataframe_long = json_prop_long.copy() # Will create a copy of the json dataframe\n",
    "xml_dataframe_long = xml_prop_long.copy() # Will create a copy of the xml dataframe\n",
    "\n",
    "prop_long_sorted = pd.concat([json_dataframe_long,xml_dataframe_long]) # Will combine both the dataframes\n",
    "prop_long_sorted.drop_duplicates(subset =\"property_id\", keep = False, inplace = True) # Will drop the rows accoording to the duplicated property ids\n",
    "#prop_long_sorted.sort_values(['property_id'],inplace=True) # Will sort the rows according to the property ids\n",
    "prop_long_sorted.reset_index(drop=True,inplace=True) # Will rest the index values to show accurate values\n",
    "\n",
    "prop_long_final = prop_long_sorted['lng'] # Will only store the latitude values\n",
    "prop_long_sorted # Will display the dataframe\n",
    "#prop_output = pd.DataFrame(columns = ['property_id', 'lat', 'lng'], data=[prop_id_final, prop_lat_final, prop_long_final])\n",
    "#prop_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 Extracting and Combining Property Addresses\n",
    "In this specific section, the `addr_street` column (attribute) was constructed by extracting all the `addresses` from the required datasets such as values from `real_state_json_data`, `real_state_xml_data_final`. The final output was then displayed using a dataframe named `json_prop_long_sorted`. The `property id` of each property was used a basis to extract the `property address` of that specific property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will extract the property ids (int) and property addresses from the json as well as xml file and will store the result into a dataframe\n",
    "json_prop_address = pd.DataFrame({'property_id': real_state_json_data['property_id'].astype(int), 'addr_street': real_state_json_data['addr_street']})\n",
    "xml_prop_address = pd.DataFrame({'property_id': real_state_xml_data_final['property_id'].astype(int), 'addr_street': real_state_xml_data_final['addr_street']})\n",
    "\n",
    "json_dataframe_address = json_prop_address.copy() # Will create a copy of the json dataframe\n",
    "xml_dataframe_address = xml_prop_address.copy() # Will create a copy of the xml dataframe\n",
    "\n",
    "prop_address_sorted = pd.concat([json_dataframe_address,xml_dataframe_address]) # Will combine both the dataframes\n",
    "prop_address_sorted.drop_duplicates(subset =\"property_id\", keep = False, inplace = True) # Will drop the rows accoording to the duplicated property ids\n",
    "#prop_address_sorted.sort_values(['property_id'],inplace=True) # Will sort the rows according to the property ids\n",
    "prop_address_sorted.reset_index(drop=True,inplace=True) # Will rest the index values to show accurate values\n",
    "\n",
    "prop_address_sorted # Will display the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5 Extracting and Combining Property Price\n",
    "In this specific section, the `price` column (attribute) was constructed by extracting all the `prices` from the required datasets such as values from `real_state_json_data`, `real_state_xml_data_final`. The final output was then displayed using a dataframe named `json_prop_long_sorted`. The `property id` of each property was used a basis to extract the `property price` of that specific property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will extract the property ids (int) and property prices from the json as well as xml file and will store the result into a dataframe\n",
    "json_prop_price = pd.DataFrame({'property_id': real_state_json_data['property_id'].astype(int), 'price': real_state_json_data['price']})\n",
    "xml_prop_price = pd.DataFrame({'property_id': real_state_xml_data_final['property_id'].astype(int), 'price': real_state_xml_data_final['price']})\n",
    "\n",
    "json_dataframe_price = json_prop_price.copy() # Will create a copy of the json dataframe\n",
    "xml_dataframe_price = xml_prop_price.copy() # Will create a copy of the xml dataframe\n",
    "\n",
    "prop_price_sorted = pd.concat([json_dataframe_price,xml_dataframe_price]) # Will combine both the dataframes\n",
    "prop_price_sorted.drop_duplicates(subset =\"property_id\", keep = False, inplace = True) # Will drop the rows accoording to the duplicated property ids\n",
    "#prop_price_sorted.sort_values(['property_id'],inplace=True) # Will sort the rows according to the property ids\n",
    "prop_price_sorted.reset_index(drop=True,inplace=True) # Will rest the index values to show accurate values\n",
    "\n",
    "prop_price_sorted # Will display the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.6 Extracting and Combining Property Type\n",
    "In this specific section, the `property_type` column (attribute) was constructed by extracting all the `property types` from the required datasets such as values from `real_state_json_data`, `real_state_xml_data_final`. The final output was then displayed using a dataframe named `json_prop_long_sorted`. The `property id` of each property was used a basis to extract the `property type` of that specific property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will extract the property ids (int) and property prices from the json as well as xml file and will store the result into a dataframe\n",
    "json_prop_typ = pd.DataFrame({'property_id': real_state_json_data['property_id'].astype(int), 'property_type': real_state_json_data['property_type']})\n",
    "xml_prop_typ = pd.DataFrame({'property_id': real_state_xml_data_final['property_id'].astype(int), 'property_type': real_state_xml_data_final['property_type']})\n",
    "\n",
    "json_dataframe_prop_typ = json_prop_typ.copy() # Will create a copy of the json dataframe\n",
    "xml_dataframe_prop_typ = xml_prop_typ.copy() # Will create a copy of the xml dataframe\n",
    "\n",
    "prop_typ_sorted = pd.concat([json_dataframe_prop_typ,xml_dataframe_prop_typ]) # Will combine both the dataframes\n",
    "prop_typ_sorted.drop_duplicates(subset =\"property_id\", keep = False, inplace = True) # Will drop the rows accoording to the duplicated property ids\n",
    "#prop_typ_sorted.sort_values(['property_id'],inplace=True) # Will sort the rows according to the property ids\n",
    "prop_typ_sorted.reset_index(drop=True,inplace=True) # Will rest the index values to show accurate values\n",
    "\n",
    "prop_typ_sorted # Will display the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.6 Extracting and Combining Property Year\n",
    "In this specific section, the `year` column (attribute) was constructed by extracting all the `property years` from the required datasets such as values from `real_state_json_data`, `real_state_xml_data_final`. The final output was then displayed using a dataframe named `json_prop_long_sorted`. The `property id` of each property was used a basis to extract the `property year` of that specific property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will extract the property ids (int) and property years from the json as well as xml file and will store the result into a dataframe\n",
    "json_year = pd.DataFrame({'property_id': real_state_json_data['property_id'].astype(int), 'year': real_state_json_data['year']})\n",
    "xml_year = pd.DataFrame({'property_id': real_state_xml_data_final['property_id'].astype(int), 'year': real_state_xml_data_final['year']})\n",
    "\n",
    "json_dataframe_year = json_year.copy() # Will create a copy of the json dataframe\n",
    "xml_dataframe_year = xml_year.copy() # Will create a copy of the xml dataframe\n",
    "\n",
    "prop_year_sorted = pd.concat([json_dataframe_year,xml_dataframe_year]) # Will combine both the dataframes\n",
    "prop_year_sorted.drop_duplicates(subset =\"property_id\", keep = False, inplace = True) # Will drop the rows accoording to the duplicated property ids\n",
    "#prop_year_sorted.sort_values(['property_id'],inplace=True) # Will sort the rows according to the property ids\n",
    "prop_year_sorted.reset_index(drop=True,inplace=True) # Will rest the index values to show accurate values\n",
    "\n",
    "prop_year_sorted # Will display the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.7 Extracting and Combining Property Bedrooms\n",
    "In this specific section, the `bedrooms` column (attribute) was constructed by extracting all the `property bedrooms` from the required datasets such as values from `real_state_json_data`, `real_state_xml_data_final`. The final output was then displayed using a dataframe named `json_prop_long_sorted`. The `property id` of each property was used a basis to extract the `number of bedrooms` of that specific property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will extract the property ids (int) and property bedrooms from the json as well as xml file and will store the result into a dataframe\n",
    "json_bedrooms = pd.DataFrame({'property_id': real_state_json_data['property_id'].astype(int), 'bedrooms': real_state_json_data['bedrooms']})\n",
    "xml_bedrooms = pd.DataFrame({'property_id': real_state_xml_data_final['property_id'].astype(int), 'bedrooms': real_state_xml_data_final['bedrooms']})\n",
    "\n",
    "json_dataframe_bedrooms = json_bedrooms.copy() # Will create a copy of the json dataframe\n",
    "xml_dataframe_bedrooms = xml_bedrooms.copy() # Will create a copy of the xml dataframe\n",
    "\n",
    "prop_bedrooms_sorted = pd.concat([json_dataframe_bedrooms,xml_dataframe_bedrooms]) # Will combine both the dataframes\n",
    "prop_bedrooms_sorted.drop_duplicates(subset =\"property_id\", keep = False, inplace = True) # Will drop the rows accoording to the duplicated property ids\n",
    "#prop_bedrooms_sorted.sort_values(['property_id'],inplace=True) # Will sort the rows according to the property ids\n",
    "prop_bedrooms_sorted.reset_index(drop=True,inplace=True) # Will rest the index values to show accurate values\n",
    "\n",
    "prop_bedrooms_sorted # Will display the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.8 Extracting and Combining Property Bathrooms\n",
    "In this specific section, the `bathrooms` column (attribute) was constructed by extracting all the `property bathrooms` from the required datasets such as values from `real_state_json_data`, `real_state_xml_data_final`. The final output was then displayed using a dataframe named `json_prop_long_sorted`. The `property id` of each property was used a basis to extract the `number of bathrooms` of that specific property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will extract the property ids (int) and property bedrooms from the json as well as xml file and will store the result into a dataframe\n",
    "json_bathrooms = pd.DataFrame({'property_id': real_state_json_data['property_id'].astype(int), 'bathrooms': real_state_json_data['bathrooms']})\n",
    "xml_bathrooms = pd.DataFrame({'property_id': real_state_xml_data_final['property_id'].astype(int), 'bathrooms': real_state_xml_data_final['bathrooms']})\n",
    "\n",
    "json_dataframe_bathrooms = json_bathrooms.copy() # Will create a copy of the json dataframe\n",
    "xml_dataframe_bathrooms = xml_bathrooms.copy() # Will create a copy of the xml dataframe\n",
    "\n",
    "prop_bathrooms_sorted = pd.concat([json_dataframe_bathrooms,xml_dataframe_bathrooms]) # Will combine both the dataframes\n",
    "prop_bathrooms_sorted.drop_duplicates(subset =\"property_id\", keep = False, inplace = True) # Will drop the rows accoording to the duplicated property ids\n",
    "#prop_bathrooms_sorted.sort_values(['property_id'],inplace=True) # Will sort the rows according to the property ids\n",
    "prop_bathrooms_sorted.reset_index(drop=True,inplace=True) # Will rest the index values to show accurate values\n",
    "\n",
    "prop_bathrooms_sorted # Will display the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.9 Extracting and Combining Parking Space\n",
    "In this specific section, the `parking_space` column (attribute) was constructed by extracting all the `parking spaces` from the required datasets such as values from `real_state_json_data`, `real_state_xml_data_final`. The final output was then displayed using a dataframe named `json_prop_long_sorted`. The `property id` of each property was used a basis to extract the `number of parking spaces` of that specific property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will extract the property ids (int) and parking spaces from the json as well as xml file and will store the result into a dataframe\n",
    "json_parking_space = pd.DataFrame({'property_id': real_state_json_data['property_id'].astype(int), 'parking_space': real_state_json_data['parking_space']})\n",
    "xml_parking_space = pd.DataFrame({'property_id': real_state_xml_data_final['property_id'].astype(int), 'parking_space': real_state_xml_data_final['parking_space']})\n",
    "\n",
    "json_dataframe_parking_space = json_parking_space.copy() # Will create a copy of the json dataframe\n",
    "xml_dataframe_parking_space = xml_parking_space.copy() # Will create a copy of the xml dataframe\n",
    "\n",
    "prop_parking_space_sorted = pd.concat([json_dataframe_parking_space,xml_dataframe_parking_space]) # Will combine both the dataframes\n",
    "prop_parking_space_sorted.drop_duplicates(subset =\"property_id\", keep = False, inplace = True) # Will drop the rows accoording to the duplicated property ids\n",
    "#prop_parking_space_sorted.sort_values(['property_id'],inplace=True) # Will sort the rows according to the property ids\n",
    "prop_parking_space_sorted.reset_index(drop=True,inplace=True) # Will rest the index values to show accurate values\n",
    "\n",
    "prop_parking_space_sorted # Will display the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.10 Finding Suburbs\n",
    "In the following subsections, the `suburb` of each property was determined by using the `Haversine Distance Forumala` which calculates the distance between two different coordinates. A function named `suburb_name` was created which takes the `laitude values` and `longitude values` as inputs and returns the names of the `closest suburb`. A new dataframe named `prop_suburb` was created to display the `suburb` of each of the property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_shp_read = shapefile.Reader(r\"VIC_LOCALITY_POLYGON_shp\") # Will read the shapefile\n",
    "shapes = prop_shp_read.shapes() # Will store the available shapes (attributes)\n",
    "record = prop_shp_read.records() # Will store the available records\n",
    "total_shapes = prop_shp_read.numRecords # Will count the total number of shapes\n",
    "\n",
    "def suburb_name(latitude,longitude): # Will be used to find and retrieve the suburb name\n",
    "    \n",
    "    point = Point(longitude,latitude) # Will take the latitude and longitude points\n",
    "    for i in range(total_shapes): # For each of the shape\n",
    "        \n",
    "        if Polygon(shapes[i].points).contains(point): # If the latitude and longitude of the property is same as that of the suburb\n",
    "            suburb_get = record[i][6] # Will get that suburb\n",
    "            return suburb_get.title() # Will return the suburb's name\n",
    "        \n",
    "    return None # Will return none\n",
    "\n",
    "prop_suburb = pd.DataFrame({'property_id': prop_id_final}) # Will create a dataframe for the suburbs\n",
    "prop_suburb['suburb'] = np.vectorize(suburb_name)(prop_lat_final,prop_long_final) # Will create a new column in that dataframe which will store the names of all the suburbs\n",
    "\n",
    "prop_suburb.head(100) # Will display the first 100 values of the output dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.11 Finding The Closest Shopping Center ID\n",
    "In the following subsections, the `closest shopping center id` of each property was determined by using the `Haversine Distance Forumala` which calculates the distance between two different coordinates (using the `latitude` and `longitude` values of the property and the shopping centers). A function named `find_distance` was used to measure the distance between the coordinates and another function named `find_closest_id` was used to find the id of the `closest shopping center`. The final output was the later displayed using a dataframe named `prop_shop_center_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_shop_id = \"not available\" # Will store the default distance value\n",
    "\n",
    "shopping_centers_data.lat = shopping_centers_data.lat.astype(float) # Will convert the string type latitude values into float\n",
    "shopping_centers_data.lng = shopping_centers_data.lng.astype(float) # Will convert the string type longitude values into float\n",
    "shopping_center_dictionary = shopping_centers_data[['sc_id','lat','lng']].set_index('sc_id').T.to_dict('list') # Will create a dictionary and for each shopping center id (key), its latitude and longitude values will be stored\n",
    "\n",
    "\n",
    "def find_distance(lat1, lon1, lat2, lon2): #Will be used to find the distance between the two points\n",
    "    \n",
    "    earth_radius = 6378 # Will store the radius of Earth in km\n",
    "    dist_lon = radians(lon2) - radians(lon1) # Will convert the float type longitude values into radians to calculate the distance\n",
    "    dist_lat = radians(lat2) - radians(lat1) # Will convert the float type latitude values into radians to calculate the distance\n",
    "    a_value = sin(dist_lat / 2)**2 + cos(radians(lat1)) * cos(radians(lat2)) * sin(dist_lon / 2)**2 # Haversine formula will be used to calculate the distance\n",
    "    c_value = 2 * atan2(sqrt(a_value), sqrt(1 - a_value)) # Will calculate the c value\n",
    "    distance = earth_radius * c_value # Will calculate the distance for each point\n",
    "    \n",
    "    return distance*1000 # Will return the distance\n",
    "\n",
    "def find_shortest_id(lat, long, shortest_id_dictionary): # Will be used to find the closest shopping center id\n",
    "    \n",
    "    closest_id_dict = {} # Will create a dictionary to store the shopping center ids\n",
    "    for key, value in shortest_id_dictionary.items(): # For each item in the dictionary\n",
    "        result = find_distance(value[0],value[1], lat, long) # Will call the find_distance function to calculate the distance\n",
    "        closest_id_dict[key] = result # Will store the ids\n",
    "        \n",
    "    return min(closest_id_dict, key=lambda key: closest_id_dict[key]) # Will return the closest shopping center id\n",
    "\n",
    "prop_shop_center_id = pd.DataFrame({'property_id': prop_id_final}) # Will create a dataframe for the shopping center ids\n",
    "\n",
    "# Will create a new column which will store the ids of the closest shopping centers by calling the find_shortest_id function\n",
    "prop_shop_center_id['Shopping_center_id'] = np.vectorize(find_shortest_id)(prop_lat_final, prop_long_final, shopping_center_dictionary) # Will call the find_shortest_id function to calculate the closest id.\n",
    "prop_shop_center_id # Will display the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.12 Finding The Closest Shopping Center Distance\n",
    "In the following subsections, the `closest shopping center distance` of each property was determined by using the `Haversine Distance Forumala` which calculates the distance between two different coordinates (using the `latitude` and `longitude` values of the property and the shopping centers). A function named `find_distance` (constructed in `section 7.11`) was used to measure the distance between the coordinates and another function named `find_closest_distance` was used to find the distance of the `closest shopping center`. The final output was the later displayed using a dataframe named `prop_shop_center_dist`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_shop_distance = 0 # Will store the default distance value\n",
    "\n",
    "def find_shortest_distance(lat, long, shortest_distance_dictionary): # Will be used to find the closest shopping center distance\n",
    "    \n",
    "    closest_dist_dict = {} # Will create a dictionary to store the shopping center distance value\n",
    "    for key, value in shortest_distance_dictionary.items(): # For each item in the dictionary\n",
    "        result = find_distance(value[0],value[1], lat, long) # Will call the find_distance function to calculate the distance\n",
    "        closest_dist_dict[key] = result # Will store the distance values\n",
    "        \n",
    "    return closest_dist_dict[min(closest_dist_dict, key=lambda key: closest_dist_dict[key])] # Will return the closest shopping center distance values\n",
    "\n",
    "prop_shop_center_dist = pd.DataFrame({'property_id': prop_id_final}) # Will create a dataframe for the shopping center distance values\n",
    "\n",
    "# Will create a new column which will store the distance values of the closest shopping centers by calling the find_shortest_distance function\n",
    "prop_shop_center_dist['Distance_to_sc'] = np.vectorize(find_shortest_distance)(prop_lat_final, prop_long_final, shopping_center_dictionary) # Will call the find_shortest_distance function to calculate the shortest distance.\n",
    "prop_shop_center_dist.Distance_to_sc = round(prop_shop_center_dist.Distance_to_sc).astype(int) # Will perform round operation on the distance values\n",
    "prop_shop_center_dist # Will display the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.13 Finding The Closest Train Station ID\n",
    "In the following subsections, the `closest train station id` of each property was determined by using the `Haversine Distance Forumala` which calculates the distance between two different coordinates (using the `latitude` and `longitude` values of the property and the train stations). A function named `find_distance` (constructed in `section 7.11`) was used to measure the distance between the coordinates and another function named `find_shortest_id` (constructed in `section 7.11`) was used to find the id of the `closest train station`. The final output was the later displayed using a dataframe named `prop_train_station_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_train_station_id = 0 # Will store the default id value\n",
    "stops_data.stop_lat = stops_data.stop_lat.astype(float) # Will convert the string type latitude values into float type\n",
    "stops_data.stop_lon = stops_data.stop_lon.astype(float) # Will convert the string type longitude values into float type\n",
    "train_station_dictionary = stops_data[['stop_id','stop_lat','stop_lon']].set_index('stop_id').T.to_dict('list') # Will create a dictionary and for each train station id (key), its latitude and longitude values will be stored\n",
    "\n",
    "prop_train_station_id = pd.DataFrame({'property_id': prop_id_final}) # Will create a dataframe for the train station ids\n",
    "\n",
    "# Will create a new column which will store the ids of the closest train stations by calling the find_shortest_id function\n",
    "prop_train_station_id['Train_station_id'] = np.vectorize(find_shortest_id)(prop_lat_final, prop_long_final, train_station_dictionary) # Will call the find_shortest_id function to calculate the closest id.\n",
    "prop_train_station_id # Will display the dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.14 Finding The Closest Train Station Distance\n",
    "In the following subsections, the `closest train station distance` of each property was determined by using the `Haversine Distance Forumala` which calculates the distance between two different coordinates (using the `latitude` and `longitude` values of the property and the train stations). A function named `find_distance` (constructed in `section 7.11`) was used to measure the distance between the coordinates and another function named `find_shortest_distance` (constructed in `section 7.12`) was used to find the distance of the `closest train station`. The final output was the later displayed using a dataframe named `prop_train_station_dist`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_train_station_distance = 0 # Will store the default distance value\n",
    "\n",
    "prop_train_station_distance = pd.DataFrame({'property_id': prop_id_final}) # Will create a dataframe for the train station distance values\n",
    "\n",
    "# Will create a new column which will store the distance values of the closest train stations by calling the find_shortest_distance function\n",
    "prop_train_station_distance['Distance_to_train_station'] = np.vectorize(find_shortest_distance)(prop_lat_final, prop_long_final, train_station_dictionary) # Will call the find_shortest_distance function to calculate the shortest distance.\n",
    "prop_train_station_distance.Distance_to_train_station = round(prop_train_station_distance.Distance_to_train_station).astype(int) # Will perform round operation on the distance values\n",
    "prop_train_station_distance # Will display the dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.15 Merging GTFS Sub-Datasets\n",
    "In the following section, the sub-datasets such as `stops_data`, `stop_times_data`, `calendar_data`, `trips_data` were combined into one by using the `merge` function of the `pandas` package. The `merge` function was used to perform `outer-join` on the `service_id` column to merge `calendar_data and trips_data`, another `outer-join` on `trip_id` to combine `calendar_data, trips_data, and stop_times_data`, and lastly, an `outer-join` to merge `stops_data` with the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtfs_dataset = pd.merge(calendar_data, trips_data, how='outer', on=['service_id']) # Will merge calendar_data and trips_data\n",
    "gtfs_dataset = pd.merge(gtfs_dataset, stop_times_data, how='outer', on=['trip_id']) # Will merge calendar_data, trips_data, and stop_times_data\n",
    "gtfs_dataset = pd.merge(gtfs_dataset, stops_data, how='outer', on=['stop_id']) # Will merge calendar_data, trips_data, stop_times_data,a and stops_data\n",
    "gtfs_dataset.head(100) # Will display the first 100 rows of the merged dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.16 Finding Average Travel Time And The Transfer Flag\n",
    "In the following section, the `gtfs_dataset` was `filtered` according to the `weekdays` (Monday to Friday) and also for those records where the `departure time` was between `7-9 AM`, and the `average travel time` from the `closest train station` of each of the available property was then calculated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.16.1 Filtering The GTFS Dataset According To The Weekdays\n",
    "In the Following sub-section, the dataframe named `gtfs_dataset` was filtered to store the data for only `weekdays` and the unnecessary columns were dropped to only have the `meaningful data`. The final output which was stored in a dataframe named `gtfs_weekdays` was then displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtfs_weekdays = gtfs_dataset[(gtfs_dataset.monday==1)&(gtfs_dataset.tuesday==1)&(gtfs_dataset.wednesday==1)&(gtfs_dataset.thursday==1)&(gtfs_dataset.friday==1)] # Will filter the dataframe named\n",
    "gtfs_weekdays.drop(['service_id', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday', 'start_date', 'end_date', 'stop_sequence', 'stop_headsign', 'pickup_type', 'drop_off_type', 'shape_dist_traveled'], axis=1, inplace=True) # Will drop the unnecessary columns\n",
    "gtfs_weekdays.reset_index(drop=True,inplace=True) # Will reset the index\n",
    "print(\"Shape: \", gtfs_weekdays.shape) # Will display the shape\n",
    "print(\"\\nLength: \", len(gtfs_weekdays)) # Will display the shape\n",
    "gtfs_weekdays.head(100) # Will display the first 100 rows of the filtered dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.16.2 Filtering The GTFS Dataset According To The Departure Time\n",
    "In the Following sub-section, the dataframe named `gtfs_dataset` was filtered to store only those where the `stop_name` was \"Flinders Street Railway Station\" and also where the `departure time` was between `7-9AM`. The final output which was stored in a dataframe named `gtfs_weekday_station_time` was then displayed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.16.2.1 Displaying The Data For All Of The Stations\n",
    "After the data was filtered according to the provided `timeframe`, the follwing section displayed the data for all of the stops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtfs_weekday_time = gtfs_weekdays[((gtfs_weekdays.departure_time>='07:00:00')&(gtfs_weekdays.departure_time<='09:00:00'))] # will filter the data according to the time (7-9AM)\n",
    "gtfs_weekday_time.head(100) # Will display the first 100 rows of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.16.2.1 Displaying The Data For The Flinders Street Railway Station\n",
    "After the data was filtered according to the provided `timeframe`, the follwing section displayed the data for the `Flinders Street Railway Station`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtfs_weekday_time_station = gtfs_weekday_time[gtfs_weekday_time['stop_name'].str.contains(\"Flinders Street Railway Station\")] # Will filter the dataset according to the stop name (Flinders Street Railway Station)\n",
    "gtfs_weekday_time_station.reset_index(drop=True,inplace=True) # Will reset the index\n",
    "\n",
    "flinders_stop_id = gtfs_weekday_time_station['stop_id'][0] # Will extract the stop id of the Flinders Street Railway Station\n",
    "print(\"Stop ID: \", flinders_stop_id)\n",
    "print(\"Length: \", len(gtfs_weekday_time_station)) # Will display the length of the dataframe\n",
    "gtfs_weekday_time_station # Will display the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.16.3 Finding Transfer Flag\n",
    "In the following section, the values for the `Transfer_flag` (0 or 1) were computed. A function named `find_transfer_flag` was used to return the values of the `Transfer_flag` column. The final out put values were then stored and displayed using a dataframe named `prop_transfer_flag`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of list to store all the stop id for each trip id, we will use this to find the transfer flag\n",
    "stop_trip_group = gtfs_weekday_time.groupby('trip_id')['stop_id'].apply(list).reset_index() # Will store the stop ids for each of the available trips (trips id)\n",
    "stop_trip_list = stop_trip_group.stop_id.tolist() # Will convert it into a list\n",
    "#print(stop_trip_list)\n",
    "def find_transfer_flag(train_id): # Will be used to find the transfer flag\n",
    "    for each_stop in stop_trip_list: # For each stop in the list of stops\n",
    "        status = False\n",
    "        if train_id in each_stop and flinders_stop_id in each_stop:\n",
    "            if each_stop.index(train_id)<=each_stop.index(flinders_stop_id):\n",
    "                return 0 # Will return 0 if a direct train exists\n",
    "    return 1 # Will return 1 if there is no direct trains\n",
    "    \n",
    "\n",
    "prop_transfer_flag = pd.DataFrame({'property_id': prop_id_final}) # Will create a dataframe for the transfer flag\n",
    "\n",
    "# Will create a new column which will store the transfer flag values (0 or 1) by calling the find_shortest_id function\n",
    "prop_transfer_flag['Transfer_flag'] = np.vectorize(find_transfer_flag)(prop_train_station_id['Train_station_id'])\n",
    "prop_transfer_flag # Will display the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.16.4 Looking For Direct Trips To The Flinders Street Railway Station\n",
    "In the following section, direct trips to the Flinders Street Railway Station were verified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flinders_direct_trip = prop_transfer_flag[(prop_transfer_flag.Transfer_flag==0)] # Will only extract the direct\n",
    "flinders_direct_list = [] # Will create an empty list which will be used to store direct trips\n",
    "flinders_direct_station = set() # Will create an empty list which will be used to store distinct direct trips\n",
    "for each_stop_id in stop_trip_list: # For each stop in the stop list\n",
    "    if flinders_stop_id in each_stop_id: # If there's a direct trip to the Flinders Street Railway Station\n",
    "        flinders_direct_list.extend(each_stop_id[:each_stop_id.index(flinders_stop_id)]) # Will append that stop id into to the flinders_direct_list list\n",
    "flinders_direct_station.update(flinders_direct_list) # Will update theset to have distinct stop values\n",
    "print(\"Stations Which Have A Direct Train To Flinders Street Railway Station:\\n\",flinders_direct_station) # Will display the those station ids which have a direct train trip to the Flinders Street Railway Station\n",
    "print(\"Total Stations: \", len(flinders_direct_station)) # Will display the total number of stations which have a direct train to the Flinders Street Railway Station"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.16.5 Estimating The Average Travel Time To CBD\n",
    "In the following section, the `average travel time` to `CBD` was calculated. First of all, the `trip_id` column and the `arrival_time` column from a dataframe named `gtfs_weekday_time_station` (Constructed in section `7.16.2.1`) was extracted and then the overall `travel time` was calculated by subtracting the `arrival time` from the `departure time` for each trip. A function named `min_travel_time_cbd` was used to calculate the `average minimum travel time` to `CBD`, and a `mean()` was responsible for calculating the `avaerage values`. These estimated values were later converted into an `integer` with the help of a `astype()` function. The final output was then loaded into a dataframe named `prop_travel_min_to_cbd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_travel_time = 0 # Will specify a default average travel time to CBD\n",
    "\n",
    "def min_travel_time_cbd(closest_train_station_id): # Will be used to calculate the minimum average time to CBD\n",
    "    return gtfs_weekday_travel_time.CBD_travel_time[(gtfs_weekday_travel_time.stop_id==closest_train_station_id)&(gtfs_weekday_travel_time.CBD_travel_time>=0)].mean()\n",
    "\n",
    "flinders_arrival_time_df = gtfs_weekday_time_station[['trip_id','arrival_time']] # Will extract the trip_id column and the arrival_time column from a dataframe\n",
    "flinders_arrival_time_df.columns = ['trip_id','flinders_arrival_time'] # Will rename the columns\n",
    "\n",
    "gtfs_weekday_travel_time = pd.merge(gtfs_weekday_time, flinders_arrival_time_df, how='outer', on=['trip_id']) # Will merge the extracted dataset with the filtered dataset\n",
    "\n",
    "gtfs_weekday_travel_time['train_dept_time'] = pd.to_datetime(gtfs_weekday_travel_time.departure_time, format='%H:%M:%S') # Will change the format of the departure time to H:M:S\n",
    "gtfs_weekday_travel_time['train_arrival_time'] = pd.to_datetime(gtfs_weekday_travel_time['flinders_arrival_time'],format= '%H:%M:%S' ) # Will change the format of the arrival time to H:M:S\n",
    "gtfs_weekday_travel_time['CBD_travel_time'] = gtfs_weekday_travel_time['train_arrival_time'].sub(gtfs_weekday_travel_time['train_dept_time'], axis=0) # Will subtract the arrival time from the departure time\n",
    "gtfs_weekday_travel_time['CBD_travel_time'] = (gtfs_weekday_travel_time['CBD_travel_time'] / np.timedelta64(1, 'm')) # Will convert it into minutes by using timedelta64() function of the numpy package\n",
    "        \n",
    "# gtfs_weekday_travel_time.head(100) # Will display the first 100 rows of the dataset\n",
    "\n",
    "prop_travel_min_to_cbd = pd.DataFrame({'property_id': prop_id_final}) # Will create a dataframe for the minimum average travel time to CBD\n",
    "\n",
    "prop_travel_min_to_cbd['travel_min_to_CBD'] = np.vectorize(min_travel_time_cbd)(prop_train_station_id['Train_station_id']) # Will call the min_travel_time_cbd function to calculate the the minimum avaerage time to cbd for each closest train station\n",
    "prop_travel_min_to_cbd = prop_travel_min_to_cbd.fillna(default_travel_time) # Will replace the extra values with the default value\n",
    "prop_travel_min_to_cbd = prop_travel_min_to_cbd.astype(int) # Will convert the minutes (from float into an integer)\n",
    "prop_travel_min_to_cbd # Will display the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.17 Finding The Closest Hospital ID\n",
    "In the following subsections, the `closest hospital id` of each property was determined by using the `Haversine Distance Forumala` which calculates the distance between two different coordinates (using the `latitude` and `longitude` values of the property and the hospitals). A function named `find_distance` (constructed in `section 7.11`) was used to measure the distance between the coordinates and another function named `find_shortest_id` (constructed in `section 7.11`) was used to find the id of the `closest hospital`. The final output was the later displayed using a dataframe named `prop_hosptal_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_hospital_id = 0 # Will store the default id value\n",
    "hospital_read_data.lat = hospital_read_data.lat.astype(float) # Will convert the string type latitude values into float type\n",
    "hospital_read_data.lng = hospital_read_data.lng.astype(float) # Will convert the string type longitude values into float type\n",
    "hospital_dictionary = hospital_read_data[['id','lat','lng']].set_index('id').T.to_dict('list') # Will create a dictionary and for each hospital id (key), its latitude and longitude values will be stored\n",
    "\n",
    "prop_hospital_id = pd.DataFrame({'property_id': prop_id_final}) # Will create a dataframe for the hospital ids\n",
    "\n",
    "# Will create a new column which will store the ids of the closest hospitals by calling the find_shortest_id function\n",
    "prop_hospital_id['Hospital_id'] = np.vectorize(find_shortest_id)(prop_lat_final, prop_long_final, hospital_dictionary) # Will call the find_shortest_id function to calculate the closest id.\n",
    "prop_hospital_id # Will display the dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.18 Finding The Closest Hospital Distance\n",
    "In the following subsections, the `closest hospital distance` of each property was determined by using the `Haversine Distance Forumala` which calculates the distance between two different coordinates (using the `latitude` and `longitude` values of the property and the hospital). A function named `find_distance` (constructed in `section 7.11`) was used to measure the distance between the coordinates and another function named `find_shortest_distance` (constructed in `section 7.12`) was used to find the distance of the `closest hospital`. The final output was the later displayed using a dataframe named `prop_hospital_dist`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_hospital_distance = 0 # Will store the default distance value\n",
    "\n",
    "prop_hospital_distance = pd.DataFrame({'property_id': prop_id_final}) # Will create a dataframe for the hospital distance values\n",
    "\n",
    "# Will create a new column which will store the distance values of the closest hospitals by calling the find_shortest_distance function\n",
    "prop_hospital_distance['Distance_to_hospital'] = np.vectorize(find_shortest_distance)(prop_lat_final, prop_long_final, hospital_dictionary) # Will call the find_shortest_distance function to calculate the shortest distance.\n",
    "prop_hospital_distance.Distance_to_hospital = round(prop_hospital_distance.Distance_to_hospital).astype(int) # Will perform round operation on the distance values\n",
    "prop_hospital_distance # Will display the dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.19 Finding The Closest Supermarket ID\n",
    "In the following subsections, the `closest supermarket id` of each property was determined by using the `Haversine Distance Forumala` which calculates the distance between two different coordinates (using the `latitude` and `longitude` values of the property and the supermarkets). A function named `find_distance` (constructed in `section 7.11`) was used to measure the distance between the coordinates and another function named `find_shortest_distance` (constructed in `section 7.12`) was used to find the id of the `closest supermarket`. The final output was the later displayed using a dataframe named `prop_supermarket_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_train_station_id = 0 # Will store the default id value\n",
    "supermarkets_data.lat = supermarkets_data.lat.astype(float) # Will convert the string type latitude values into float type\n",
    "supermarkets_data.lng = supermarkets_data.lng.astype(float) # Will convert the string type longitude values into float type\n",
    "supermarkets_dictionary = supermarkets_data[['id','lat','lng']].set_index('id').T.to_dict('list') # Will create a dictionary and for each supermarket id (key), its latitude and longitude values will be stored\n",
    "\n",
    "prop_supermarket_id = pd.DataFrame({'property_id': prop_id_final}) # Will create a dataframe for each supermarkets ids\n",
    "\n",
    "# Will create a new column which will store the ids of the closest supermarket by calling the find_shortest_id function\n",
    "prop_supermarket_id['Supermarket_id'] = np.vectorize(find_shortest_id)(prop_lat_final, prop_long_final, supermarkets_dictionary)\n",
    "prop_supermarket_id # Will display the dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.20 Finding The Closest Supermarket Distance\n",
    "In the following subsections, the `closest supermarket distance` of each property was determined by using the `Haversine Distance Forumala` which calculates the distance between two different coordinates (using the `latitude` and `longitude` values of the property and the supermarkets). A function named `find_distance` (constructed in `section 7.11`) was used to measure the distance between the coordinates and another function named `find_shortest_distance` (constructed in `section 7.12`) was used to find the distance of the `closest supermarket`. The final output was the later displayed using a dataframe named `prop_supermarket_dist`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_supermarket_distance = 0 # Will store the default distance value\n",
    "\n",
    "prop_supermarket_distance = pd.DataFrame({'property_id': prop_id_final}) # Will create a dataframe for the supermarket distance values\n",
    "\n",
    "# Will create a new column which will store the distance values of the closest supermarkets by calling the find_shortest_distance function\n",
    "prop_supermarket_distance['Distance_to_supermaket'] = np.vectorize(find_shortest_distance)(prop_lat_final, prop_long_final, supermarkets_dictionary)\n",
    "prop_supermarket_distance.Distance_to_supermaket = round(prop_supermarket_distance.Distance_to_supermaket).astype(int) # Will perform round operation on the distance values\n",
    "prop_supermarket_distance # Will display the dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.21 Integrating All Of The Extracted Datasets Into One\n",
    "In the following section, every data which was calculated and extracted in the previous sections was then coverted into a single dataframe (`property_data_integration`) using the `pandas` package. The same dataframe was then converted into a final `CSV` formatted file named `dhrruvtokas_solution.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will create a single dataframe for all of the dataframes\n",
    "property_data_integration = pd.DataFrame({'Property_id': prop_id_final, 'lat': prop_lat_sorted['lat'], 'lng': prop_long_sorted['lng'], 'addr_street': prop_address_sorted['addr_street'], 'suburb': prop_suburb['suburb'], 'price': prop_price_sorted['price'], 'property_type': prop_typ_sorted['property_type'], 'year': prop_year_sorted['year'], 'bedrooms': prop_bedrooms_sorted['bedrooms'], 'bathrooms': prop_bathrooms_sorted['bathrooms'], 'parking_space': prop_parking_space_sorted['parking_space'], 'Shopping_center_id': prop_shop_center_id['Shopping_center_id'], 'Distance_to_sc': prop_shop_center_dist['Distance_to_sc'], 'Train_station_id': prop_train_station_id['Train_station_id'], 'Distance_to_train_station': prop_train_station_distance['Distance_to_train_station'], 'travel_min_to_CBD': prop_travel_min_to_cbd['travel_min_to_CBD'], 'Transfer_flag': prop_transfer_flag['Transfer_flag'], 'Hospital_id': prop_hospital_id['Hospital_id'], 'Distance_to_hospital': prop_hospital_distance['Distance_to_hospital'], 'Supermarket_id': prop_supermarket_id['Supermarket_id'], 'Distance_to_supermaket': prop_supermarket_distance['Distance_to_supermaket']})\n",
    "property_data_integration['price'] = property_data_integration['price'].astype(int)\n",
    "property_data_integration.to_csv ('dhrruvtokas_solution.csv', index = False, header=True) # Will convert the final dataframe into an output csv file\n",
    "property_data_integration_read = pd.read_csv(\"dhrruvtokas_solution.csv\") # Will open the csv file to read \n",
    "property_data_integration_read # Will display the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.21.1 Final Dataset Description\n",
    "In the following sub-section, a dunction named `describe()` was used to provide a small description of the `final dataset` which was `integrated` in the previous section (`section 7.21`). Since the values of some columns (`Distance_to_sc`, `travel_min_to_CBD`, and `Distance_to_hospital`) were `scaled` on a differnt scale in comparison with the others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_data_integration.describe() # Will describe the final output dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task2: DATA RESHAPING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Data Reshaping\n",
    "In the following sub-section, the dataset will be `reshaped` by using a set of `Normalization/Transformation` methods.\n",
    "* Brief Explaination: \n",
    "1. Root Transformation: This transformation was performed by using `np.sqrt()` function to determine the `postive square root` value of the `input data value`. The resulting `tranformed` value of each property was a `float type`.\n",
    "2. Power Transformation: This transformation technique (performed using `np.power()` function) is similar to that of `Box-cox` transformation, the purpose of this transformation was to `transform` the input `price` data into the `Gaussian` like probability distribution.\n",
    "3. Log Transformation: This transformation (performed using `np.log()` function) was used to calculate `Natural Logarithm` for each of the 'input data value' and transform the data based on those values.\n",
    "4. Z-Score Normalization: Calculating Z-Score (performed by using `StandardScaler()` function) is same as that of calculating `Standard Score`. This was carried out to determine exactly how far each `data point` was from the `mean value`.\n",
    "5. MinMax Normalization: This transformation (performed by using `MinMaxScaler()` function) is used when the values of attribute are on a `different scale` which could have resulted into a `poor prediction model` and it is precisely why `MinMax Normalisation` was used to bring all those values on the `same scale`.  \n",
    "6. Box-cox Transformation: To check whether the `non-normal distribution` had been `transformed` successfully, it is necessary to check the level of the `normality`, however, `box-cox` transformation, never checks for it, so the normality is `not guaranted`, and that is precisely where the `probability plot` comes into the picture, to verify the `normality`.\n",
    "\n",
    "Each of these `suitable` transformation techniques were carried out on the `price` of each property, `distance from the nearest shopping center`,`minimum travel time to CBD`, and `distance from the nearest hospital` respectively. Some plots were also used in appropriate sections to display how the data looks after the transformation. In addition to this, separate dataframes were created to reflect the changes which have been made throughout the following sub-sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Deal With Mean Values\n",
    "In the following section, the mean values of `price` column, `Distance_to_sc` column, `travel_min_to_CBD` column and `Distance_to_hospital` column were calculated (by using a function named `mean()`) respectively.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_mean = property_data_integration['price'].mean() # Will calculate the mean value based on all the values of the price column\n",
    "shopping_center_mean = property_data_integration['Distance_to_sc'].mean() # Will calculate the mean value based on all the values of the Distance_to_sc column\n",
    "cbd_mean = property_data_integration['travel_min_to_CBD'].mean() # Will calculate the mean value based on all the values of the travel_min_to_CBD column\n",
    "hospital_mean = property_data_integration['Distance_to_hospital'].mean() # Will calculate the mean value based on all the values of the Distance_to_hospital column\n",
    "print(\"Mean Of The Price Column: \", price_mean) # Will display the mean value of the price column\n",
    "print(\"Mean Of The Distance_to_sc Column: \", shopping_center_mean) # Will display the mean value of the Distance_to_sc column\n",
    "print(\"Mean Of The travel_min_to_CBD Column: \", cbd_mean) # Will display the mean value of the travel_min_to_CBD column\n",
    "print(\"Mean Of The Distance_to_hospital Column: \", hospital_mean) # Will display the mean value of the Distance_to_hospital column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Basic Normalization/Transformation Techniques On The Price Column\n",
    "In the following section, basic transformation techniques such as `Root Transformation`, `Power Transformation`, `Log Transformation`, `Z-Score Normalisation`, and `MinMax Normalisation` were applied on the values of the `price` column of the final dataset (`property_data_integration`). The resulting values were then stored into a dataframe named `price_reshape_data` so the `actual price` can be compared with the `transformed` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_data = property_data_integration # Willstore the final output dataframe\n",
    "price_reshape_data = pd.DataFrame({'property_id': prop_id_final, 'price': prop_price_sorted['price'].astype(float)}) # Will create a dataframe for the transformation values\n",
    "\n",
    "price_reshape_data['root_on_price'] = np.sqrt(price_reshape_data.price) # Will apply root transformation on the price column\n",
    "\n",
    "price_reshape_data['power_on_price'] = np.power(price_reshape_data.price,2) # Will apply power transformation on the price column\n",
    "\n",
    "price_reshape_data['log_on_price'] = np.log(price_reshape_data.price) # Will apply log transformation on the price column\n",
    "\n",
    "price_standard_scale = preprocessing.StandardScaler().fit(price_reshape_data[['price']]) # Will use the StandardScaler() function to fit the price data to apply Z-Score Normalization\n",
    "price_standard_scale_df = price_standard_scale.transform(price_reshape_data[['price']]) # Will transform the data\n",
    "price_standard_scale_df[0:5] # Will slice the require data\n",
    "price_reshape_data['z_score_on_price'] = price_standard_scale_df[:,0] # Will store the required values into a new column\n",
    "\n",
    "price_minmax_scale = preprocessing.MinMaxScaler().fit(price_reshape_data[['price']]) # Will use the MinMaxScaler() function to apply MinMax Normalization\n",
    "price_minmax_scale_df = price_minmax_scale.transform(price_reshape_data[['price']]) # Will transform the data\n",
    "price_reshape_data['minmax_on_price'] = price_minmax_scale_df[:,0] # Will store the required values into a new column\n",
    "\n",
    "price_reshape_data # Will display the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.1 Reviewing The Normalized/Transformed Data\n",
    "In the following section, the values which were calculated in the previous section (section `8.2`) by using `Normalization/Tranformation` were plotted with the help of `matplot` package and the `boxcox()` function. A `for loop` was used to plot a `histogram` for each column, a legend in the `upper-left` corner of the plot was specified by using a function named `legend`, and plots were `rescaled` by using a set of functions named `set_figheight` and `set_figwidth` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_figure, price_axis = plt.subplots(nrows=2, ncols=3, figsize=(30, 20)) # Specifying the dimensions of the plot\n",
    "\n",
    "price_reshape_data_columns = ['price','root_on_price','power_on_price','log_on_price','z_score_on_price','minmax_on_price'] # Will specify the columns for the plots\n",
    "for each_axis, each_column in zip_longest(price_axis.flat, price_reshape_data_columns): # For each column in the column names\n",
    "    each_axis.hist(price_reshape_data[each_column], bins=50, edgecolor='blue') # Will plot the histogram\n",
    "    each_axis.set_title(\"Graphical Histogram For: \" +each_column + \" column\") # Will set the title for each of the histogram\n",
    "\n",
    "# This part of the code will be used to perform Boxcox Transformation  \n",
    "price_box_data, price_lambda_value = stats.boxcox(property_data_integration['price'].astype(float)) # Will transform the property price and will also save the lambda value\n",
    "price_figure_box, price_axis_box = plt.subplots(1, 2) # Will create axes for the plots\n",
    "  \n",
    "# Will plot the original property prices\n",
    "sns.distplot(property_data_integration['price'].astype(float), hist = False, kde = True, kde_kws = {'shade': True, 'linewidth': 2}, label = \"Non-Normal Distribution\", color =\"red\", ax = price_axis_box[0]) \n",
    "# Will plot the transformed property prices\n",
    "sns.distplot(price_box_data, hist = False, kde = True, kde_kws = {'shade': True, 'linewidth': 2}, label = \"Normal Distribution\", color =\"blue\", ax = price_axis_box[1])  \n",
    "\n",
    "plt.legend(loc = \"upper left\") # Will add legends to the plots \n",
    "\n",
    "price_figure_box.set_figheight(5) # Will be used to rescale the plots\n",
    "price_figure_box.set_figwidth(15) # Will be used to rescale the plots\n",
    "\n",
    "print(\"Lambda Value For Property price Transformation: \", price_lambda_value) # Will display the lambda value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.2 Data Description\n",
    "In the following sub-section, a function named `describe()` was used to provide a small description of the `transformed data` (named `price_reshape_data`) which was discovered in the previous section (`section 8.2`). In addition to this, the mean value of the transformed data (by using the data from `StandardScaler()`) was also calculated by using a `mean()` function. The mean values which were calculated before this transformation are available in `section 8.1`. The `standard deviation` of the `price` column was `5.94` (available in the `section 7.21.1`) before any kind of transformation was done, and after the transformation, the standard deviation was found out to be `1.0` (Available in the section below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean Before The Transformation: \", price_mean) # Will display the mean before any transformation was done\n",
    "print(\"Mean Value After The Transformation: \", price_reshape_data['z_score_on_price'].mean()) # Will calculate the mean value after the transformation\n",
    "price_reshape_data.describe() # Will describe the transformed dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 Basic Normalization/Transformation Techniques On The Distance_to_sc Column\n",
    "In the following section, basic transformation techniques such as `Root Transformation`, `Power Transformation`, `Log Transformation`, `Z-Score Normalisation`, and `MinMax Normalisation` were applied on the values of the `Distance_to_sc` column of the final dataset (`property_data_integration`). The resulting values were then stored into a dataframe named `shopping_reshape_data` so the `actual distance` can be compared with the `transformed` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shopping_reshape_data = pd.DataFrame({'property_id': prop_id_final, 'Distance_to_sc': property_data_integration['Distance_to_sc'].astype(float)}) # Will create a dataframe for the transformation values\n",
    "\n",
    "shopping_reshape_data['root_on_shopping'] = np.sqrt(shopping_reshape_data.Distance_to_sc) # Will apply root transformation on the Distance_to_sc column\n",
    "\n",
    "shopping_reshape_data['power_on_shopping'] = np.power(shopping_reshape_data.Distance_to_sc,2) # Will apply power transformation on the Distance_to_sc column\n",
    "\n",
    "shopping_reshape_data['log_on_shopping'] = np.log(shopping_reshape_data.Distance_to_sc) # Will apply log transformation on the Distance_to_sc column\n",
    "\n",
    "shop_standard_scale = preprocessing.StandardScaler().fit(shopping_reshape_data[['Distance_to_sc']]) # Will use the StandardScaler() function to fit the Distance_to_sc data to apply Z-Score Normalization\n",
    "shop_standard_scale_df = shop_standard_scale.transform(shopping_reshape_data[['Distance_to_sc']]) # Will transform the data\n",
    "shop_standard_scale_df[0:5] # Will slice the require data\n",
    "shopping_reshape_data['z_score_on_shopping'] = shop_standard_scale_df[:,0] # Will store the required values into a new column\n",
    "\n",
    "shop_minmax_scale = preprocessing.MinMaxScaler().fit(shopping_reshape_data[['Distance_to_sc']]) # Will use the MinMaxScaler() function to apply MinMax Normalization\n",
    "shop_minmax_scale_df = shop_minmax_scale.transform(shopping_reshape_data[['Distance_to_sc']]) # Will transform the data\n",
    "shopping_reshape_data['minmax_on_shopping'] = shop_minmax_scale_df[:,0] # Will store the required values into a new column\n",
    "\n",
    "shopping_reshape_data # Will display the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3.1 Reviewing The Normalized/Transformed Data\n",
    "In the following section, the values which were calculated in the previous section (`section 8.3`) by using `Normalization/Tranformation` were plotted with the help of `matplot` package and the `boxcox()` function. A `for loop` was used to plot a `histogram` for each column, a legend in the `upper-left` corner of the plot was specified by using a function named `legend`, and plots were `rescaled` by using a set of functions named `set_figheight` and `set_figwidth` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_figure, shop_axis = plt.subplots(nrows=2, ncols=3, figsize=(30, 20)) # Specifying the dimensions of the plot\n",
    "\n",
    "shop_reshape_data_columns = ['Distance_to_sc','root_on_shopping','power_on_shopping','log_on_shopping','z_score_on_shopping','minmax_on_shopping'] # Will specify the columns for the plots\n",
    "for each_axis_shop, each_column_shop in zip_longest(shop_axis.flat, shop_reshape_data_columns): # For each column in the column names\n",
    "    each_axis_shop.hist(shopping_reshape_data[each_column_shop], bins=50, edgecolor='blue') # Will plot the histogram\n",
    "    each_axis_shop.set_title(\"Graphical Histogram For: \" +each_column_shop + \" column\") # Will set the title for each of the histogram\n",
    "\n",
    "# This part of the code will be used to perform Boxcox Transformation  \n",
    "shopping_box_data, shopping_lambda_value = stats.boxcox(property_data_integration['Distance_to_sc'].astype(float)) # Will transform the shopping center distance values and will also save the lambda value\n",
    "shopping_figure_box, shopping_axis_box = plt.subplots(1, 2) # Will create axes for the plots\n",
    "  \n",
    "# Will plot the original shopping center distance values\n",
    "sns.distplot(property_data_integration['Distance_to_sc'].astype(float), hist = False, kde = True, kde_kws = {'shade': True, 'linewidth': 2}, label = \"Non-Normal Distribution\", color =\"red\", ax = shopping_axis_box[0]) \n",
    "# Will plot the transformed shopping center distance values\n",
    "sns.distplot(shopping_box_data, hist = False, kde = True, kde_kws = {'shade': True, 'linewidth': 2}, label = \"Normal Distribution\", color =\"blue\", ax = shopping_axis_box[1])  \n",
    "\n",
    "plt.legend(loc = \"upper left\") # Will add legends to the plots \n",
    "\n",
    "shopping_figure_box.set_figheight(5) # Will be used to rescale the plots\n",
    "shopping_figure_box.set_figwidth(15) # Will be used to rescale the plots\n",
    "\n",
    "print(\"Lambda Value For Shopping Center Distance Transformation: \", shopping_lambda_value) # Will display the lambda value    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3.2 Data Description\n",
    "In the following sub-section, a function named `describe()` was used to provide a small description of the `transformed data` (named `shopping_reshape_data`) which was discovered in the previous section (`section 8.3`). In addition to this, the mean value of the transformed data was also calculated by using a `mean()` function. The mean values which were calculated before this transformation are available in `section 8.1`. The `standard deviation` of the `Distance_to_sc` column was `1384.24` (available in the `section 7.21.1`) before any kind of transformation was done, and after the transformation, the standard deviation was found out to be `1.0` (Available in the section below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean Before The Transformation: \", shopping_center_mean) # Will display the mean before any transformation was done\n",
    "print(\"Mean Value After The Transformation: \", shopping_reshape_data['z_score_on_shopping'].mean()) # Will calculate the mean value after the transformation\n",
    "shopping_reshape_data.describe() # Will describe the transformed dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4 Basic Normalization/Transformation Techniques On The travel_min_to_CBD Column\n",
    "In the following section, basic transformation techniques such as `Root Transformation`, `Power Transformation`, `Z-Score Normalisation`, and `MinMax Normalisation` were applied on the values of the `travel_min_to_CBD` column of the final dataset (`property_data_integration`). The resulting values were then stored into a dataframe named `cbd_reshape_data` so the `actual distance` can be compared with the `transformed` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbd_reshape_data = pd.DataFrame({'property_id': prop_id_final, 'travel_min_to_CBD': property_data_integration['travel_min_to_CBD'].astype(float)}) # Will create a dataframe for the transformation values\n",
    "\n",
    "cbd_reshape_data['root_on_cbd'] = np.sqrt(cbd_reshape_data.travel_min_to_CBD) # Will apply root transformation on the travel_min_to_CBD column\n",
    "\n",
    "cbd_reshape_data['power_on_cbd'] = np.power(cbd_reshape_data.travel_min_to_CBD,2) # Will apply power transformation on the travel_min_to_CBD column\n",
    "\n",
    "#cbd_reshape_data['log_on_cbd'] = np.log(cbd_reshape_data.travel_min_to_CBD) # Will apply log transformation on the travel_min_to_CBD column\n",
    "\n",
    "cbd_standard_scale = preprocessing.StandardScaler().fit(cbd_reshape_data[['travel_min_to_CBD']]) # Will use the StandardScaler() function to fit the travel_min_to_CBD data to apply Z-Score Normalization\n",
    "cbd_standard_scale_df = cbd_standard_scale.transform(cbd_reshape_data[['travel_min_to_CBD']]) # Will transform the data\n",
    "cbd_standard_scale_df[0:5] # Will slice the require data\n",
    "cbd_reshape_data['z_score_on_cbd'] = cbd_standard_scale_df[:,0] # Will store the required values into a new column\n",
    "\n",
    "cbd_minmax_scale = preprocessing.MinMaxScaler().fit(cbd_reshape_data[['travel_min_to_CBD']]) # Will use the MinMaxScaler() function to apply MinMax Normalization\n",
    "cbd_minmax_scale_df = cbd_minmax_scale.transform(cbd_reshape_data[['travel_min_to_CBD']]) # Will transform the data\n",
    "cbd_reshape_data['minmax_on_cbd'] = cbd_minmax_scale_df[:,0] # Will store the required values into a new column\n",
    "\n",
    "cbd_reshape_data # Will display the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4.1 Reviewing The Normalized/Transformed Data\n",
    "In the following section, the values which were calculated in the previous section (section `8.4`) by using `Normalization/Tranformation` were plotted with the help of `matplot` package and the `boxcox()` function. A `for loop` was used to plot a `histogram` for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbd_figure, cbd_axis = plt.subplots(nrows=2, ncols=3, figsize=(30, 20)) # Specifying the dimensions of the plot\n",
    "\n",
    "cbd_reshape_data_columns = ['travel_min_to_CBD','root_on_cbd','power_on_cbd','z_score_on_cbd','minmax_on_cbd'] # Will specify the columns for the plots\n",
    "\n",
    "try: # Will check for errors\n",
    "    for each_axis_cbd, each_column_cbd in zip_longest(cbd_axis.flat, cbd_reshape_data_columns): # For each column in the column names\n",
    "        each_axis_cbd.hist(cbd_reshape_data[each_column_cbd], bins=50, edgecolor='blue') # Will plot the histogram\n",
    "        each_axis_cbd.set_title(\"Graphical Histogram For: \" +each_column_cbd + \" column\") # Will set the title for each of the histogram\n",
    "except: # If there's any exception, then pass\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4.2 Data Description\n",
    "In the following sub-section, a function named `describe()` was used to provide a small description of the `transformed data` (named `cbd_reshape_data`) which was discovered in the previous section (`section 8.4`). In addition to this, the mean value of the transformed data was also calculated by using a `mean()` function. The mean values which were calculated before this transformation are available in `section 8.1`. The `standard deviation` of the `travel_min_to_CBD` column was `12.28` (available in the `section 7.21.1`) before any kind of transformation was done, and after the transformation, the standard deviation was found out to be `1.0` (Available in the section below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean Before The Transformation: \", cbd_mean) # Will display the mean before any transformation was done\n",
    "print(\"Mean Value After The Transformation: \", cbd_reshape_data['z_score_on_cbd'].mean()) # Will calculate the mean value after the transformation\n",
    "cbd_reshape_data.describe() # Will describe the transformed dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.5 Basic Normalization/Transformation Techniques On The Distance_to_hospital Column\n",
    "In the following section, basic transformation techniques such as `Root Transformation`, `Power Transformation`, `Log Transformation`, `Z-Score Normalisation`, and `MinMax Normalisation` were applied on the values of the `Distance_to_hospital` column of the final dataset (`property_data_integration`). The resulting values were then stored into a dataframe named `hospital_reshape_data` so the `actual distance` can be compared with the `transformed` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_reshape_data = pd.DataFrame({'property_id': prop_id_final, 'Distance_to_hospital': property_data_integration['Distance_to_hospital'].astype(float)}) # Will create a dataframe for the transformation values\n",
    "\n",
    "hospital_reshape_data['root_on_hospital'] = np.sqrt(hospital_reshape_data.Distance_to_hospital) # Will apply root transformation on the Distance_to_hospital column\n",
    "\n",
    "hospital_reshape_data['power_on_hospital'] = np.power(hospital_reshape_data.Distance_to_hospital,2) # Will apply power transformation on the Distance_to_hospital column\n",
    "\n",
    "hospital_reshape_data['log_on_hospital'] = np.log(hospital_reshape_data.Distance_to_hospital) # Will apply log transformation on the Distance_to_hospital column\n",
    "\n",
    "hospital_standard_scale = preprocessing.StandardScaler().fit(hospital_reshape_data[['Distance_to_hospital']]) # Will use the StandardScaler() function to fit the Distance_to_hospital data to apply Z-Score Normalization\n",
    "hospital_standard_scale_df = hospital_standard_scale.transform(hospital_reshape_data[['Distance_to_hospital']]) # Will transform the data\n",
    "hospital_standard_scale_df[0:5] # Will slice the require data\n",
    "hospital_reshape_data['z_score_on_hospital'] = hospital_standard_scale_df[:,0] # Will store the required values into a new column\n",
    "\n",
    "hospital_minmax_scale = preprocessing.MinMaxScaler().fit(hospital_reshape_data[['Distance_to_hospital']]) # Will use the MinMaxScaler() function to apply MinMax Normalization\n",
    "hospital_minmax_scale_df = hospital_minmax_scale.transform(hospital_reshape_data[['Distance_to_hospital']]) # Will transform the data\n",
    "hospital_reshape_data['minmax_on_hospital'] = hospital_minmax_scale_df[:,0] # Will store the required values into a new column\n",
    "\n",
    "hospital_reshape_data # Will display the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.1 Reviewing The Normalized/Transformed Data\n",
    "In the following section, the values which were calculated in the previous section (section `8.5`) by using `Normalization/Tranformation` were plotted with the help of `matplot` package and the `boxcox()` function. A `for loop` was used to plot a `histogram` for each column, a legend in the `upper-left` corner of the plot was specified by using a function named `legend`, and plots were `rescaled` by using a set of functions named `set_figheight` and `set_figwidth` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_figure, hospital_axis = plt.subplots(nrows=2, ncols=3, figsize=(30, 20)) # Specifying the dimensions of the plot\n",
    "\n",
    "hospital_reshape_data_columns = ['Distance_to_hospital','root_on_hospital','power_on_hospital','log_on_hospital','z_score_on_hospital','minmax_on_hospital'] # Will specify the columns for the plots\n",
    "for each_axis_hospital, each_column_hospital in zip_longest(hospital_axis.flat, hospital_reshape_data_columns): # For each column in the column names\n",
    "    each_axis_hospital.hist(hospital_reshape_data[each_column_hospital], bins=50, edgecolor='blue') # Will plot the histogram\n",
    "    each_axis_hospital.set_title(\"Graphical Histogram For: \" +each_column_hospital + \" column\") # Will set the title for each of the histogram\n",
    "\n",
    "# This part of the code will be used to perform Boxcox Transformation  \n",
    "hospital_box_data, hospital_lambda_value = stats.boxcox(property_data_integration['Distance_to_hospital'].astype(float)) # Will transform the hospital distance values and will also save the lambda value\n",
    "hospital_figure_box, hospital_axis_box = plt.subplots(1, 2) # Will create axes for the plots\n",
    "  \n",
    "# Will plot the original hospital distance values\n",
    "sns.distplot(property_data_integration['Distance_to_hospital'].astype(float), hist = False, kde = True, kde_kws = {'shade': True, 'linewidth': 2}, label = \"Non-Normal Distribution\", color =\"red\", ax = hospital_axis_box[0]) \n",
    "# Will plot the transformed hospital distance values\n",
    "sns.distplot(hospital_box_data, hist = False, kde = True, kde_kws = {'shade': True, 'linewidth': 2}, label = \"Normal Distribution\", color =\"blue\", ax = hospital_axis_box[1])  \n",
    "\n",
    "plt.legend(loc = \"upper left\") # Will add legends to the plots \n",
    "\n",
    "hospital_figure_box.set_figheight(5) # Will be used to rescale the plots\n",
    "hospital_figure_box.set_figwidth(15) # Will be used to rescale the plots\n",
    "\n",
    "print(\"Lambda Value For Shopping Center Distance Transformation: \", hospital_lambda_value) # Will display the lambda value    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.2 Data Description\n",
    "In the following sub-section, a function named `describe()` was used to provide a small description of the `transformed data` (named `hospital_reshape_data`) which was discovered in the previous section (`section 8.5`). In addition to this, the mean value of the transformed data was also calculated by using a `mean()` function. The mean values which were calculated before this transformation are available in `section 8.1`. The `standard deviation` of the `Distance_to_hospital` column was `1754.84` (available in the `section 7.21.1`) before any kind of transformation was done, and after the transformation, the standard deviation was found out to be `1.0` (Available in the section below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean Before The Transformation: \", hospital_mean) # Will display the mean before any transformation was done\n",
    "print(\"Mean Value After The Transformation: \", hospital_reshape_data['z_score_on_hospital'].mean()) # Will calculate the mean value after the transformation\n",
    "hospital_reshape_data.describe() # Will describe the transformed dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.6 Predicting Property Price Using A Linear Model\n",
    "In the following section, a `Linear Model` was created with the help of `sklearn` package to predict the price of each of the property."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.6.1 Fitting The Dependent And Independent Attributes Into A Linear Model\n",
    "In the following sub-section, the independent variables were `fitted` into a `linear regression model` with the help of `reg` (`LinearRegression()`) function and the `intercept` (`reg.intercept_`) and the `coefficient` (`reg.coef_`) were displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression() # Will be used to create a linear model\n",
    "reg.fit(property_data_integration[['Distance_to_sc','travel_min_to_CBD','Distance_to_hospital']],property_data_integration['price']) # Will fit the dependent and independent variables into the model\n",
    "print(\"Intercept: \", reg.intercept_) # Will display the intercept\n",
    "print(\"Coefficient: \", reg.coef_) # Will display the coeffcients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.6.2 Predicting The Price Of Each Property\n",
    "In the following section, the `LinearRegression` function which was created in the previous section (`section 8.6.1`) was used in this section to `predict` (by using a `predict()` function) the `price` of each `property` using a `Linear Model` (named `reg`, was created in the `section 8.5.1`). The model takes following `independent variables` as input:\n",
    "* Distance to the nearest shopping center (`Distance_to_sc`).\n",
    "* Minimum average time taken to travel to the CBD (`travel_min_to_CBD`).\n",
    "* Distance to the nearest hospital (`Distance_to_hospital`).\n",
    "\n",
    "The main aim of this `Linear Model` is to `predict` the `price` (`dependent variable`) of each property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_property_price = [] # Will create an empty dictionary which will be used store all the predicted values \n",
    "\n",
    "for nearest_shopping_center, min_travel_time_cbd, nearest_hospital, property_price_value in zip_longest(property_data_integration['Distance_to_sc'],property_data_integration['travel_min_to_CBD'],property_data_integration['Distance_to_hospital'],property_data_integration['price']): # For all the dependent variables in the Distance_to_sc, travel_min_to_CBD, Distance_to_hospital, and independent variable named price\n",
    "    predicted_price = reg.predict([[nearest_shopping_center,min_travel_time_cbd,nearest_hospital]]) # Will predict the value \n",
    "    predict_property_price.append(predicted_price.item()) # Will extract the item from the array and that item (predicted value) will be appended into the predict_property_price list\n",
    "\n",
    "# Will create a dataframe to show the difference between the original price and the predicted price\n",
    "predicted_property_price_data = pd.DataFrame({'property_id': prop_id_final, 'Distance_to_sc': property_data_integration['Distance_to_sc'], 'travel_min_to_CBD': property_data_integration['travel_min_to_CBD'], 'Distance_to_hospital': property_data_integration['Distance_to_hospital'], 'price': property_data_integration['price'], 'predicted_price': predict_property_price})\n",
    "predicted_property_price_data['predicted_price'] = predicted_property_price_data['predicted_price'].astype(int) # Will convert the price of each property into an integer\n",
    "\n",
    "predicted_property_price_data # Will display the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.6.2.1 Creating A Linear Model For The Z-Score Normalization Values\n",
    "In the following sub-section, this time, `Z-Score Normalization` values of independent variables (`z_score_on_shopping`, `z_score_on_cbd`, `z_score_on_hospital`) and the independent variable (`z_score_on_price`) were `fitted` (by using a `fit()` function) into a `Linear Model` (named `reg_z_score`) to `predict` the `property price`. The `predicted` values were first appended into a list named `predict_property_price_z_score`n and then this list was later converted into a column of a dataframe named `predicted_property_price_data_z_score`. At the end of this section, the `predicted property price` values were displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will create a dataframe to combine to combine all z-score values into one \n",
    "z_score_price_shop_cbd_hosp = pd.DataFrame({'property_id': prop_id_final, 'z_score_on_shopping': shopping_reshape_data['z_score_on_shopping'], 'z_score_on_cbd': cbd_reshape_data['z_score_on_cbd'], 'z_score_on_hospital': hospital_reshape_data['z_score_on_hospital'], 'z_score_on_price': price_reshape_data['z_score_on_price']})\n",
    "\n",
    "reg_z_score = LinearRegression() # Will be used to create a linear model\n",
    "reg_z_score.fit(z_score_price_shop_cbd_hosp[['z_score_on_shopping','z_score_on_cbd','z_score_on_hospital']],z_score_price_shop_cbd_hosp['z_score_on_price']) # Will fit the dependent and independent variables into the model\n",
    "\n",
    "print(reg_z_score.intercept_) # Will display the intercept\n",
    "print(reg_z_score.coef_) # Will display the coeffcients\n",
    "\n",
    "predict_property_price_z_score = [] # Will create an empty dictionary which will be used store all the predicted values \n",
    "\n",
    "for nearest_shopping_center_z, min_travel_time_cbd_z, nearest_hospital_z, property_price_value_z in zip_longest(z_score_price_shop_cbd_hosp['z_score_on_shopping'],z_score_price_shop_cbd_hosp['z_score_on_cbd'],z_score_price_shop_cbd_hosp['z_score_on_hospital'],z_score_price_shop_cbd_hosp['z_score_on_price']): # For all the dependent variables in the z_score_on_shopping, z_score_on_cbd, z_score_on_hospital, and independent variable named z_score_on_price\n",
    "    predicted_price_z_score = reg_z_score.predict([[nearest_shopping_center_z, min_travel_time_cbd_z, nearest_hospital_z]]) # Will predict the value \n",
    "    predict_property_price_z_score.append(predicted_price_z_score.item()) # Will extract the item from the array and that item (predicted value) will be appended into the predict_property_price list\n",
    "\n",
    "# Will create a dataframe to show the difference between the original price and the predicted price\n",
    "predicted_property_price_data_z_score = pd.DataFrame({'property_id': prop_id_final, 'z_score_on_shopping': shopping_reshape_data['z_score_on_shopping'], 'z_score_on_cbd': cbd_reshape_data['z_score_on_cbd'], 'z_score_on_hospital': hospital_reshape_data['z_score_on_hospital'], 'z_score_on_price': price_reshape_data['z_score_on_price'], 'predicted_price_z_score': predict_property_price_z_score})\n",
    "\n",
    "predicted_property_price_data_z_score # Will display the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.7 Applying Z-Score Normalization\n",
    "In the following section, unlike previous sections, `Z-Score Normalization` was applied (by using a `StandardScaler()` function) to columns named `price`, `Distance_to_sc`, `travel_min_to_CBD`, and 'Distance_to_hospital' respectively. The data was then transformed by using a `transform()` fucntion, and the `Z-Score` values of these columns were then `fitted` into a `Linear Model` which was created in the `section 8.7.1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_score_normalization = preprocessing.StandardScaler().fit(property_data_integration[['price', 'Distance_to_sc','travel_min_to_CBD','Distance_to_hospital']]) # Will call the StandardScaler() function to fit the data\n",
    "z_score_normalization_df = z_score_normalization.transform(property_data_integration[['price', 'Distance_to_sc','travel_min_to_CBD','Distance_to_hospital']]) # Will transform the data\n",
    "z_score_normalization_df[0:5] # Will slice the data\n",
    "z_score_reshape_data = pd.DataFrame({'property_id': prop_id_final}) # Will create a dataframe for the z-score values\n",
    "\n",
    "z_score_reshape_data['price_z_score'] = z_score_normalization_df[:,0] # Will extract the transformed data into a new column\n",
    "z_score_reshape_data['Distance_to_sc_z_score'] = z_score_normalization_df[:,1] # Will extract the transformed data into a new column\n",
    "z_score_reshape_data['travel_min_to_CBD_z_score'] = z_score_normalization_df[:,2] # Will extract the transformed data into a new column\n",
    "z_score_reshape_data['Distance_to_hospital_z_score'] = z_score_normalization_df[:,3] # Will extract the transformed data into a new column\n",
    "\n",
    "# Will be used to plot the results of the Z-Score Normalization\n",
    "z_score_reshape_data['price_z_score'].plot(), z_score_reshape_data['Distance_to_sc_z_score'], z_score_reshape_data['travel_min_to_CBD_z_score'].plot(), z_score_reshape_data['Distance_to_hospital_z_score'].plot()\n",
    "z_score_reshape_data # Will display the transformed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.7.1 Creating A Linear Model To Fit The Transformed Data (Z-Score Normalization)\n",
    "In the following sub-section, the data which was transformed in the previous section (`section 8.7`) was then `trained` by using the `train_test_split()` function, and was `fitted` into a `Linear Model` by using a `fit()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model_z_score = LinearRegression() # Will be used to create a linear model\n",
    "# Will be used to train the data\n",
    "X_train, x_test, Y_train, y_test = train_test_split(z_score_reshape_data[['Distance_to_sc_z_score','travel_min_to_CBD_z_score','Distance_to_hospital_z_score']], z_score_reshape_data['price_z_score'],random_state=111)\n",
    "linear_model_z_score.fit(X_train,Y_train)  # Will fir the trained data into the Linear Model\n",
    "\n",
    "print(\"R-Squared Value\",linear_model_z_score.score(x_test,y_test)) # Will display the score\n",
    "print(\"Intercept: \", linear_model_z_score.intercept_) # Will display the intercept\n",
    "print('Coefficients: ', linear_model_z_score.coef_) # Will display the coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.8 Applying MinMax Normalization\n",
    "In the following section, unlike previous sections, `MinMax` was applied (by using a `MinMaxScaler()` function) to columns named `price`, `Distance_to_sc`, `travel_min_to_CBD`, and 'Distance_to_hospital' respectively. The data was then transformed by using a `transform()` fucntion, and the `MinMax` values of these columns were then `fitted` into a `Linear Model` which was created in the `section 8.8.1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax_normalization = preprocessing.MinMaxScaler().fit(property_data_integration[['price', 'Distance_to_sc','travel_min_to_CBD','Distance_to_hospital']]) # Will call the StandardScaler() function to fit the data\n",
    "minmax_normalization_df = minmax_normalization.transform(property_data_integration[['price', 'Distance_to_sc','travel_min_to_CBD','Distance_to_hospital']]) # Will transform the data\n",
    "minmax_normalization_df[0:5] # Will slice the data\n",
    "minmax_reshape_data = pd.DataFrame({'property_id': prop_id_final}) # Will create a dataframe for the MinMax values\n",
    "\n",
    "minmax_reshape_data['price_minmax'] = minmax_normalization_df[:,0] # Will extract the transformed data into a new column\n",
    "minmax_reshape_data['Distance_to_sc_minmax'] = minmax_normalization_df[:,1] # Will extract the transformed data into a new column\n",
    "minmax_reshape_data['travel_min_to_CBD_minmax'] = minmax_normalization_df[:,2] # Will extract the transformed data into a new column\n",
    "minmax_reshape_data['Distance_to_hospital_minmax'] = minmax_normalization_df[:,3] # Will extract the transformed data into a new column\n",
    "\n",
    "# Will be used to plot the results of the MinMax Normalization\n",
    "minmax_reshape_data['price_minmax'].plot(), minmax_reshape_data['Distance_to_sc_minmax'], minmax_reshape_data['travel_min_to_CBD_minmax'].plot(), minmax_reshape_data['Distance_to_hospital_minmax'].plot()\n",
    "minmax_reshape_data # Will display the transformed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.8.1 Creating A Linear Model To Fit The Transformed Data (MinMax Normalization)\n",
    "In the following sub-section, the data which was transformed in the previous section (`section 8.8`) was then trained by using the `train_test_split()` function, and was `fitted` into a `Linear Model` by using a `fit()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model_minmax = LinearRegression() # Will be used to create a linear model\n",
    "# Will be used to train the data\n",
    "X_train, x_test, Y_train, y_test = train_test_split(minmax_reshape_data[['Distance_to_sc_minmax','travel_min_to_CBD_minmax','Distance_to_hospital_minmax']], minmax_reshape_data['price_minmax'],random_state=111)\n",
    "linear_model_minmax.fit(X_train,Y_train)  # Will fir the trained data into the Linear Model\n",
    "\n",
    "print(\"R-Squared Value\",linear_model_minmax.score(x_test,y_test)) # Will display the score\n",
    "print(\"Intercept: \", linear_model_minmax.intercept_) # Will display the intercept\n",
    "print('Coefficients: ', linear_model_minmax.coef_) # Will display the coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Conclusion\n",
    "I was given a total of `3` dataset files (`.zip files`), each of which contained several other sub-datasets. The first dataset named `dhrruvtokas.zip` contained `hospitals.xlsx`, `real_state.json`, `real_state.xlsx` ,`shoppingcenter.pdf`, and `supermarkets.html` respectively. These files were extracted from the `.zip` structure in the `section 3.1` and their data was later extracted in the `section 4.1`. The second dataset named `GTFS_Melbourne_Train_Information.zip` contained a total of `8 text files`, these files were `agency.txt`, `calendar.txt`, `calendar_dates.txt`, `routes.txt`, `shapes.txt`, `stop_times.txt`, `stops.txt`, and `trips.txt` respectively. These files were examined and extracted in the `section 5`. The last dataset named `vic_suburb_boundary.zip` contained a total of `4 shape files` which included `VIC_LOCALITY_POLYGON_shp.dbf`, `VIC_LOCALITY_POLYGON_shp.prj`, `VIC_LOCALITY_POLYGON_shp.shp`, and `VIC_LOCALITY_POLYGON_shp.shx` respectively. These files were later reviewed and were used in the later sections to estimate certain `distance values`. In the `section 7`, the required data was calaculated and extracted for each of the output columns. For each column, its extracted data set was then loaded into a dataframe, and all togethere these sums up to be `21 dataframes`. These dataframes were later `combined` into a `single dataframe` which was then converted into a single `CSV` formatted file (`dhrruvtokas_solution.csv`), and this was achieved in the `section 7.21`. After The data was `reshaped` (in `section 8`) by using a set of `Normalization/Transformation` techniques used in the same section, the `property price` of each property was successfully `predicted` and displayed using a `Linear Model`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. References\n",
    "1. Maurizio Lenzerini. (2002). `Data Integration: A Theoretical Perspective`. A paper published at Proceedings of the Twenty-first ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems Conference. Retrieved from https://www.researchgate.net/publication/220266329_Data_Integration_A_Theoretical_Perspective\n",
    "\n",
    "2. Patrick Ziegler & Klaus R. Dittrich. (2007). `Data Integration  Problems, Approaches, and Perspectives`. A paper published in Conceptual Modelling in Information Systems Engineering Book. Retrieved from https://link.springer.com/chapter/10.1007/978-3-540-72677-7_3\n",
    "\n",
    "3. Yigal Arens, Chin Y. Chee, Chun-Nan Hsu, &Craig A. Knoblock. (1993). `Retrieving and Integrating Data from Multiple Information Sources`. An article published in International Journal of Cooperative Information System. Retrieved from https://www.researchgate.net/publication/220095189_Retrieving_and_Integrating_Data_from_Multiple_Information_Sources\n",
    "\n",
    "4. Yulia Kostyuchenko. (2018). `Analysis of approaches to data modeling using Python libraries`. A paper published at ITMO University Scientific and Educational Conference. Retrieved from https://www.researchgate.net/publication/335600632_Analysis_of_approaches_to_data_modeling_using_Python_libraries\n",
    "\n",
    "5. Sankranti Srinivasa Rao. (2020). `Stock Prediction Analysis by using Linear Regression Machine Learning Algorithm`. An article published in International Journal of Innovative Technology and Exploring Engineering (IJITEE). Retrieved from https://www.ijitee.org/wp-content/uploads/papers/v9i4/D1110029420.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
